{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Regularization - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that you had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with your previous machine learning work, you should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no \n",
    "\n",
    "In this lab, you'll use the a train-validate-test partition to get better insights of how to tune neural networks using regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, you'll see how to include a validation set. From there, you'll define and compile the model like before. However, this time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test set but also the validation set.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Apply dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing you'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras import models, layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before you begin to practice some of your new tools regarding regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding your complaint text\n",
    "* Transforming your category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since you have quite a bit of data and training networks takes a substantial amount of time and resources, downsample in order to test your initial pipeline. Going forward, these can be interesting areas of investigation: how does your model's performance change as you increase (or decrease) the size of your dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, you need to do some preprocessing and data manipulation before building the neural network. \n",
    "\n",
    "Keep the 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences type: <class 'list'>\n",
      "one_hot_results type: <class 'numpy.ndarray'>\n",
      "Found 21613 unique tokens.\n",
      "Dimensions of our coded results: (10000, 2000)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=2000) #Initialize a tokenizer.\n",
    "tokenizer.fit_on_texts(complaints) #Fit it to the complaints\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(complaints) #Generate sequences\n",
    "print('sequences type:', type(sequences))\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary') #Similar to sequences, but returns a numpy array\n",
    "print('one_hot_results type:', type(one_hot_results))\n",
    "word_index = tokenizer.word_index #Useful if we wish to decode later\n",
    "\n",
    "print(f'Found {len(word_index)} unique tokens.') #Tokens are the number of unique words across the corpus\n",
    "print('Dimensions of our coded results:', np.shape(one_hot_results)) #Our coded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "> **Note**: This is similar to your previous work with dummy variables. Each of the various product categories will be its own column, and each observation will be a row. In turn, each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder() #Initialize. `le` = `LabelEncoder`\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product)  \n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! \n",
    "Below, perform an appropriate train test split.\n",
    "> Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, you saw that in deep learning, you generally set aside a validation set, which is then used during hyperparameter tuning. Afterwards, when you have decided upon a final model, the test set can then be used to define the final model performance. \n",
    "\n",
    "In this example, take the first 1000 cases out of the training set to create a validation set. You should do this for both `X_train` and `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because you are dealing with a multiclass problem (classifying the complaints into 7 classes), use a softmax classifyer in order to output 7 class probabilities per case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=50, activation='relu', input_shape=(2000,)))\n",
    "model.add(Dense(units=25, activation='relu'))\n",
    "model.add(Dense(units=7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train your model! Note that this is where you also introduce the validation data to the model. Train the model for 120 epochs in mini-batches of 256 samples. This time, include the argument `validation_data=(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.9519 - acc: 0.1476 - val_loss: 1.9395 - val_acc: 0.1690\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.9288 - acc: 0.1709 - val_loss: 1.9213 - val_acc: 0.1800\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9112 - acc: 0.1955 - val_loss: 1.9046 - val_acc: 0.2070\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8938 - acc: 0.2203 - val_loss: 1.8863 - val_acc: 0.2290\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8743 - acc: 0.2396 - val_loss: 1.8655 - val_acc: 0.2370\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8521 - acc: 0.2605 - val_loss: 1.8409 - val_acc: 0.2520\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8264 - acc: 0.2791 - val_loss: 1.8134 - val_acc: 0.2660\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7971 - acc: 0.3020 - val_loss: 1.7825 - val_acc: 0.2840\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7645 - acc: 0.3243 - val_loss: 1.7488 - val_acc: 0.3080\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7291 - acc: 0.3484 - val_loss: 1.7130 - val_acc: 0.3350\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6912 - acc: 0.3743 - val_loss: 1.6744 - val_acc: 0.3510\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6511 - acc: 0.3963 - val_loss: 1.6337 - val_acc: 0.3770\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6088 - acc: 0.4199 - val_loss: 1.5915 - val_acc: 0.4100\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.5652 - acc: 0.4521 - val_loss: 1.5487 - val_acc: 0.4420\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.5196 - acc: 0.4780 - val_loss: 1.5049 - val_acc: 0.4750\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.4725 - acc: 0.5123 - val_loss: 1.4601 - val_acc: 0.4960\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4245 - acc: 0.5352 - val_loss: 1.4145 - val_acc: 0.5200\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3751 - acc: 0.5651 - val_loss: 1.3669 - val_acc: 0.5560\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3244 - acc: 0.5887 - val_loss: 1.3201 - val_acc: 0.5770\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2738 - acc: 0.6121 - val_loss: 1.2744 - val_acc: 0.5980\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2232 - acc: 0.6303 - val_loss: 1.2282 - val_acc: 0.6130\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1747 - acc: 0.6484 - val_loss: 1.1862 - val_acc: 0.6230\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1287 - acc: 0.6627 - val_loss: 1.1417 - val_acc: 0.6500\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0851 - acc: 0.6751 - val_loss: 1.1041 - val_acc: 0.6580\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0446 - acc: 0.6864 - val_loss: 1.0666 - val_acc: 0.6700\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0075 - acc: 0.6960 - val_loss: 1.0352 - val_acc: 0.6650\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9730 - acc: 0.7040 - val_loss: 1.0064 - val_acc: 0.6710\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9408 - acc: 0.7111 - val_loss: 0.9776 - val_acc: 0.6790\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9115 - acc: 0.7208 - val_loss: 0.9517 - val_acc: 0.6870\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8845 - acc: 0.7244 - val_loss: 0.9300 - val_acc: 0.6920\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8594 - acc: 0.7312 - val_loss: 0.9074 - val_acc: 0.6970\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8370 - acc: 0.7368 - val_loss: 0.8929 - val_acc: 0.6910\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8159 - acc: 0.7407 - val_loss: 0.8715 - val_acc: 0.7040\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7962 - acc: 0.7453 - val_loss: 0.8560 - val_acc: 0.7060\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7782 - acc: 0.7492 - val_loss: 0.8410 - val_acc: 0.7090\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7613 - acc: 0.7539 - val_loss: 0.8302 - val_acc: 0.7090\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7461 - acc: 0.7572 - val_loss: 0.8161 - val_acc: 0.7110\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7315 - acc: 0.7625 - val_loss: 0.8028 - val_acc: 0.7140\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7179 - acc: 0.7645 - val_loss: 0.7929 - val_acc: 0.7190\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7053 - acc: 0.7692 - val_loss: 0.7815 - val_acc: 0.7220\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.6933 - acc: 0.7741 - val_loss: 0.7751 - val_acc: 0.7250\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6822 - acc: 0.7733 - val_loss: 0.7714 - val_acc: 0.7160\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6716 - acc: 0.7757 - val_loss: 0.7573 - val_acc: 0.7270\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6616 - acc: 0.7788 - val_loss: 0.7507 - val_acc: 0.7240\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.6526 - acc: 0.7808 - val_loss: 0.7437 - val_acc: 0.7300\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6434 - acc: 0.7841 - val_loss: 0.7410 - val_acc: 0.7260\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6344 - acc: 0.7841 - val_loss: 0.7328 - val_acc: 0.7330\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6263 - acc: 0.7885 - val_loss: 0.7275 - val_acc: 0.7230\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6186 - acc: 0.7891 - val_loss: 0.7238 - val_acc: 0.7320\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6112 - acc: 0.7916 - val_loss: 0.7188 - val_acc: 0.7350\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6037 - acc: 0.7944 - val_loss: 0.7156 - val_acc: 0.7340\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5963 - acc: 0.7985 - val_loss: 0.7080 - val_acc: 0.7370\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5899 - acc: 0.8000 - val_loss: 0.7063 - val_acc: 0.7300\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5832 - acc: 0.8017 - val_loss: 0.7002 - val_acc: 0.7370\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5772 - acc: 0.8040 - val_loss: 0.6972 - val_acc: 0.7360\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5710 - acc: 0.8053 - val_loss: 0.6977 - val_acc: 0.7390\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5653 - acc: 0.8065 - val_loss: 0.6913 - val_acc: 0.7370\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5591 - acc: 0.8089 - val_loss: 0.6872 - val_acc: 0.7360\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5533 - acc: 0.8139 - val_loss: 0.6886 - val_acc: 0.7360\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5482 - acc: 0.8149 - val_loss: 0.6814 - val_acc: 0.7450\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5429 - acc: 0.8141 - val_loss: 0.6825 - val_acc: 0.7420\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5375 - acc: 0.8179 - val_loss: 0.6769 - val_acc: 0.7380\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5327 - acc: 0.8211 - val_loss: 0.6770 - val_acc: 0.7360\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5282 - acc: 0.8216 - val_loss: 0.6746 - val_acc: 0.7380\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5232 - acc: 0.8231 - val_loss: 0.6723 - val_acc: 0.7420\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5187 - acc: 0.8248 - val_loss: 0.6684 - val_acc: 0.7490\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5141 - acc: 0.8261 - val_loss: 0.6679 - val_acc: 0.7440\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5094 - acc: 0.8272 - val_loss: 0.6654 - val_acc: 0.7470\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5050 - acc: 0.8288 - val_loss: 0.6638 - val_acc: 0.7420\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5007 - acc: 0.8293 - val_loss: 0.6622 - val_acc: 0.7460\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4965 - acc: 0.8319 - val_loss: 0.6603 - val_acc: 0.7500\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4924 - acc: 0.8333 - val_loss: 0.6623 - val_acc: 0.7480\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4880 - acc: 0.8344 - val_loss: 0.6574 - val_acc: 0.7540\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4844 - acc: 0.8360 - val_loss: 0.6568 - val_acc: 0.7550\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4802 - acc: 0.8373 - val_loss: 0.6559 - val_acc: 0.7450\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4761 - acc: 0.8391 - val_loss: 0.6571 - val_acc: 0.7430\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4725 - acc: 0.8412 - val_loss: 0.6542 - val_acc: 0.7530\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4690 - acc: 0.8424 - val_loss: 0.6520 - val_acc: 0.7610\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4653 - acc: 0.8444 - val_loss: 0.6514 - val_acc: 0.7610\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4615 - acc: 0.8469 - val_loss: 0.6502 - val_acc: 0.7570\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4578 - acc: 0.8483 - val_loss: 0.6511 - val_acc: 0.7540\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4545 - acc: 0.8489 - val_loss: 0.6504 - val_acc: 0.7480\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4511 - acc: 0.8511 - val_loss: 0.6500 - val_acc: 0.7480\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4474 - acc: 0.8516 - val_loss: 0.6474 - val_acc: 0.7600\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4447 - acc: 0.8533 - val_loss: 0.6452 - val_acc: 0.7580\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4406 - acc: 0.8556 - val_loss: 0.6483 - val_acc: 0.7500\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4373 - acc: 0.8559 - val_loss: 0.6490 - val_acc: 0.7510\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4340 - acc: 0.8569 - val_loss: 0.6447 - val_acc: 0.7590\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4309 - acc: 0.8585 - val_loss: 0.6432 - val_acc: 0.7550\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4274 - acc: 0.8611 - val_loss: 0.6440 - val_acc: 0.7550\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4244 - acc: 0.8605 - val_loss: 0.6428 - val_acc: 0.7610\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4215 - acc: 0.8619 - val_loss: 0.6439 - val_acc: 0.7570\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4181 - acc: 0.8647 - val_loss: 0.6471 - val_acc: 0.7530\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4152 - acc: 0.8629 - val_loss: 0.6408 - val_acc: 0.7570\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4118 - acc: 0.8668 - val_loss: 0.6408 - val_acc: 0.7530\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4090 - acc: 0.8679 - val_loss: 0.6442 - val_acc: 0.7550\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4060 - acc: 0.8716 - val_loss: 0.6429 - val_acc: 0.7610\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4029 - acc: 0.8712 - val_loss: 0.6413 - val_acc: 0.7580\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4003 - acc: 0.8707 - val_loss: 0.6418 - val_acc: 0.7590\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3974 - acc: 0.8709 - val_loss: 0.6411 - val_acc: 0.7560\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.3943 - acc: 0.8740 - val_loss: 0.6413 - val_acc: 0.7580\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.3919 - acc: 0.8743 - val_loss: 0.6417 - val_acc: 0.7580\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3888 - acc: 0.8765 - val_loss: 0.6408 - val_acc: 0.7600\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3860 - acc: 0.8775 - val_loss: 0.6448 - val_acc: 0.7570\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.3836 - acc: 0.8789 - val_loss: 0.6409 - val_acc: 0.7560\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3805 - acc: 0.8804 - val_loss: 0.6404 - val_acc: 0.7580\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.3780 - acc: 0.8788 - val_loss: 0.6418 - val_acc: 0.7580\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3752 - acc: 0.8815 - val_loss: 0.6516 - val_acc: 0.7480\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3730 - acc: 0.8803 - val_loss: 0.6421 - val_acc: 0.7540\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3698 - acc: 0.8835 - val_loss: 0.6431 - val_acc: 0.7590\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.3670 - acc: 0.8843 - val_loss: 0.6449 - val_acc: 0.7520\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.3652 - acc: 0.8820 - val_loss: 0.6407 - val_acc: 0.7580\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3623 - acc: 0.8867 - val_loss: 0.6427 - val_acc: 0.7550\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.3600 - acc: 0.8871 - val_loss: 0.6447 - val_acc: 0.7540\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.3576 - acc: 0.8893 - val_loss: 0.6414 - val_acc: 0.7530\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.3549 - acc: 0.8892 - val_loss: 0.6414 - val_acc: 0.7560\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.3522 - acc: 0.8896 - val_loss: 0.6412 - val_acc: 0.7570\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3498 - acc: 0.8911 - val_loss: 0.6447 - val_acc: 0.7520\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.3475 - acc: 0.8925 - val_loss: 0.6431 - val_acc: 0.7570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.3448 - acc: 0.8927 - val_loss: 0.6450 - val_acc: 0.7560\n"
     ]
    }
   ],
   "source": [
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 35us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 37us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.34647279361089073, 0.8918666666984558],\n",
       " [0.6714394076665242, 0.7546666661898295])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train, results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! you remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function versus the number of epochs. Be sure to include the training and the validation loss in the same plot. Then, create a second plot comparing training and validation accuracy to the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a limit around the 60th epoch. This means that you're probably **overfitting** the model to the training data when you train for many epochs past this dropoff point of around 40 epochs. Luckily, you learned how to tackle overfitting in the previous lecture! Since it seems clear that you are training too long, include early stopping at the 60th epoch first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "Below, observe how to update the model to include an earlier cutoff point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9496 - acc: 0.1569 - val_loss: 1.9307 - val_acc: 0.1970\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9219 - acc: 0.1935 - val_loss: 1.9069 - val_acc: 0.2280\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9018 - acc: 0.2224 - val_loss: 1.8865 - val_acc: 0.2430\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8828 - acc: 0.2384 - val_loss: 1.8677 - val_acc: 0.2550\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8628 - acc: 0.2555 - val_loss: 1.8465 - val_acc: 0.2660\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8398 - acc: 0.2688 - val_loss: 1.8230 - val_acc: 0.2770\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8132 - acc: 0.2867 - val_loss: 1.7960 - val_acc: 0.2960\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7831 - acc: 0.3069 - val_loss: 1.7637 - val_acc: 0.3130\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7495 - acc: 0.3223 - val_loss: 1.7293 - val_acc: 0.3340\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7129 - acc: 0.3456 - val_loss: 1.6920 - val_acc: 0.3500\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6741 - acc: 0.3599 - val_loss: 1.6532 - val_acc: 0.3670\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6331 - acc: 0.3856 - val_loss: 1.6137 - val_acc: 0.3840\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5901 - acc: 0.4104 - val_loss: 1.5716 - val_acc: 0.4180\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5457 - acc: 0.4348 - val_loss: 1.5280 - val_acc: 0.4420\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5006 - acc: 0.4628 - val_loss: 1.4845 - val_acc: 0.4770\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4545 - acc: 0.4879 - val_loss: 1.4391 - val_acc: 0.5120\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4072 - acc: 0.5173 - val_loss: 1.3921 - val_acc: 0.5460\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3585 - acc: 0.5492 - val_loss: 1.3458 - val_acc: 0.5670\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3097 - acc: 0.5768 - val_loss: 1.3001 - val_acc: 0.5880\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2624 - acc: 0.6015 - val_loss: 1.2574 - val_acc: 0.6040\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2172 - acc: 0.6215 - val_loss: 1.2179 - val_acc: 0.6160\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1747 - acc: 0.6364 - val_loss: 1.1768 - val_acc: 0.6350\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1347 - acc: 0.6532 - val_loss: 1.1400 - val_acc: 0.6440\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0971 - acc: 0.6640 - val_loss: 1.1070 - val_acc: 0.6560\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0617 - acc: 0.6773 - val_loss: 1.0744 - val_acc: 0.6580\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0281 - acc: 0.6893 - val_loss: 1.0478 - val_acc: 0.6700\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9975 - acc: 0.6952 - val_loss: 1.0176 - val_acc: 0.6760\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9681 - acc: 0.7028 - val_loss: 0.9921 - val_acc: 0.6860\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9408 - acc: 0.7116 - val_loss: 0.9666 - val_acc: 0.6920\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9153 - acc: 0.7159 - val_loss: 0.9436 - val_acc: 0.7020\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8916 - acc: 0.7237 - val_loss: 0.9229 - val_acc: 0.7020\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8693 - acc: 0.7288 - val_loss: 0.9051 - val_acc: 0.7060\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8483 - acc: 0.7331 - val_loss: 0.8861 - val_acc: 0.7090\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8292 - acc: 0.7393 - val_loss: 0.8701 - val_acc: 0.7180\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8115 - acc: 0.743 - 0s 32us/step - loss: 0.8110 - acc: 0.7432 - val_loss: 0.8566 - val_acc: 0.7200\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7940 - acc: 0.7455 - val_loss: 0.8417 - val_acc: 0.7160\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7777 - acc: 0.7488 - val_loss: 0.8287 - val_acc: 0.7270\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7632 - acc: 0.7517 - val_loss: 0.8162 - val_acc: 0.7250\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7484 - acc: 0.7573 - val_loss: 0.8037 - val_acc: 0.7280\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7355 - acc: 0.7605 - val_loss: 0.7937 - val_acc: 0.7330\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7227 - acc: 0.7637 - val_loss: 0.7836 - val_acc: 0.7380\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7109 - acc: 0.7667 - val_loss: 0.7762 - val_acc: 0.7350\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7000 - acc: 0.7700 - val_loss: 0.7663 - val_acc: 0.7430\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6891 - acc: 0.7724 - val_loss: 0.7589 - val_acc: 0.7380\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6785 - acc: 0.7720 - val_loss: 0.7513 - val_acc: 0.7440\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6692 - acc: 0.7772 - val_loss: 0.7447 - val_acc: 0.7460\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6596 - acc: 0.7791 - val_loss: 0.7408 - val_acc: 0.7370\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6513 - acc: 0.7817 - val_loss: 0.7315 - val_acc: 0.7470\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6426 - acc: 0.7835 - val_loss: 0.7264 - val_acc: 0.7470\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6343 - acc: 0.7891 - val_loss: 0.7211 - val_acc: 0.7490\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.6267 - acc: 0.7901 - val_loss: 0.7178 - val_acc: 0.7460\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6192 - acc: 0.7917 - val_loss: 0.7118 - val_acc: 0.7490\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6122 - acc: 0.7929 - val_loss: 0.7089 - val_acc: 0.7430\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6046 - acc: 0.7975 - val_loss: 0.7031 - val_acc: 0.7560\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5980 - acc: 0.7979 - val_loss: 0.7009 - val_acc: 0.7460\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5913 - acc: 0.8025 - val_loss: 0.6986 - val_acc: 0.7500\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5848 - acc: 0.8033 - val_loss: 0.6938 - val_acc: 0.7500\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5792 - acc: 0.8051 - val_loss: 0.6914 - val_acc: 0.7490\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5730 - acc: 0.8083 - val_loss: 0.6870 - val_acc: 0.7500\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5675 - acc: 0.8097 - val_loss: 0.6845 - val_acc: 0.7550\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 35us/step\n",
      "1500/1500 [==============================] - 0s 36us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5628536545753479, 0.8124], [0.738668150583903, 0.7346666669845581])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)\n",
    "results_train, results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! your test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs model you originally fit.\n",
    "\n",
    "Now, take a look at how regularization techniques can further improve your model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, take a look at L2 regularization. Keras makes L2 regularization easy. Simply add the `kernel_regularizer=kernel_regulizers.l2(lamda_coeff)` parameter to any model layer. The `lambda_coeff` parameter determines the strength of the regularization you wish to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 2.5912 - acc: 0.1584 - val_loss: 2.5743 - val_acc: 0.1760\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.5613 - acc: 0.1851 - val_loss: 2.5484 - val_acc: 0.2010\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.5367 - acc: 0.2084 - val_loss: 2.5242 - val_acc: 0.2340\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.5114 - acc: 0.2288 - val_loss: 2.4982 - val_acc: 0.2590\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.4837 - acc: 0.2544 - val_loss: 2.4700 - val_acc: 0.2810\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.4528 - acc: 0.2851 - val_loss: 2.4378 - val_acc: 0.3080\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.4177 - acc: 0.3160 - val_loss: 2.4020 - val_acc: 0.3390\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.3785 - acc: 0.3532 - val_loss: 2.3622 - val_acc: 0.3750\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.3358 - acc: 0.3888 - val_loss: 2.3190 - val_acc: 0.3930\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.2901 - acc: 0.4141 - val_loss: 2.2749 - val_acc: 0.4260\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.2423 - acc: 0.4592 - val_loss: 2.2285 - val_acc: 0.4510\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.1926 - acc: 0.4983 - val_loss: 2.1807 - val_acc: 0.4670\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.1418 - acc: 0.5299 - val_loss: 2.1322 - val_acc: 0.4860\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.0907 - acc: 0.5549 - val_loss: 2.0840 - val_acc: 0.5040\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.0396 - acc: 0.5757 - val_loss: 2.0355 - val_acc: 0.5230\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.9895 - acc: 0.5952 - val_loss: 1.9915 - val_acc: 0.5480\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9407 - acc: 0.6165 - val_loss: 1.9447 - val_acc: 0.5530\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8933 - acc: 0.6265 - val_loss: 1.9033 - val_acc: 0.5720\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8482 - acc: 0.6405 - val_loss: 1.8632 - val_acc: 0.5910\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8052 - acc: 0.6516 - val_loss: 1.8239 - val_acc: 0.5960\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7644 - acc: 0.6548 - val_loss: 1.7857 - val_acc: 0.6050\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7255 - acc: 0.6704 - val_loss: 1.7501 - val_acc: 0.6060\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6890 - acc: 0.6791 - val_loss: 1.7173 - val_acc: 0.6210\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6549 - acc: 0.6875 - val_loss: 1.6859 - val_acc: 0.6360\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6226 - acc: 0.6975 - val_loss: 1.6570 - val_acc: 0.6350\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5924 - acc: 0.7024 - val_loss: 1.6313 - val_acc: 0.6510\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5639 - acc: 0.7111 - val_loss: 1.6040 - val_acc: 0.6560\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5377 - acc: 0.7161 - val_loss: 1.5843 - val_acc: 0.6680\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5127 - acc: 0.7208 - val_loss: 1.5598 - val_acc: 0.6750\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4897 - acc: 0.7263 - val_loss: 1.5428 - val_acc: 0.6590\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4678 - acc: 0.7285 - val_loss: 1.5226 - val_acc: 0.6780\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4473 - acc: 0.7329 - val_loss: 1.5059 - val_acc: 0.6810\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4277 - acc: 0.7372 - val_loss: 1.4864 - val_acc: 0.6840\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4099 - acc: 0.7416 - val_loss: 1.4715 - val_acc: 0.6960\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3923 - acc: 0.7464 - val_loss: 1.4574 - val_acc: 0.6930\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3762 - acc: 0.7487 - val_loss: 1.4431 - val_acc: 0.6900\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3608 - acc: 0.7520 - val_loss: 1.4315 - val_acc: 0.6990\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3464 - acc: 0.7544 - val_loss: 1.4167 - val_acc: 0.6960\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3322 - acc: 0.7584 - val_loss: 1.4063 - val_acc: 0.7040\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3193 - acc: 0.7616 - val_loss: 1.3949 - val_acc: 0.7010\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3068 - acc: 0.7656 - val_loss: 1.3842 - val_acc: 0.7010\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2948 - acc: 0.7664 - val_loss: 1.3738 - val_acc: 0.7030\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2830 - acc: 0.7708 - val_loss: 1.3680 - val_acc: 0.7090\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2723 - acc: 0.7687 - val_loss: 1.3571 - val_acc: 0.7120\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2614 - acc: 0.7724 - val_loss: 1.3519 - val_acc: 0.7100\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2516 - acc: 0.7744 - val_loss: 1.3407 - val_acc: 0.7150\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2413 - acc: 0.7751 - val_loss: 1.3326 - val_acc: 0.7040\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2319 - acc: 0.7789 - val_loss: 1.3232 - val_acc: 0.7120\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2227 - acc: 0.7805 - val_loss: 1.3193 - val_acc: 0.7220\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2135 - acc: 0.7821 - val_loss: 1.3117 - val_acc: 0.7140\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2053 - acc: 0.7848 - val_loss: 1.3058 - val_acc: 0.7150\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1965 - acc: 0.7864 - val_loss: 1.2997 - val_acc: 0.7130\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1882 - acc: 0.7892 - val_loss: 1.2908 - val_acc: 0.7160\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1803 - acc: 0.7932 - val_loss: 1.2875 - val_acc: 0.7140\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1726 - acc: 0.7921 - val_loss: 1.2817 - val_acc: 0.7250\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1649 - acc: 0.7939 - val_loss: 1.2768 - val_acc: 0.7310\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1574 - acc: 0.7975 - val_loss: 1.2699 - val_acc: 0.7240\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1503 - acc: 0.7972 - val_loss: 1.2648 - val_acc: 0.7210\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1434 - acc: 0.7992 - val_loss: 1.2597 - val_acc: 0.7250\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1358 - acc: 0.7995 - val_loss: 1.2528 - val_acc: 0.7260\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1293 - acc: 0.8027 - val_loss: 1.2473 - val_acc: 0.7310\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1226 - acc: 0.8053 - val_loss: 1.2482 - val_acc: 0.7340\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1162 - acc: 0.8043 - val_loss: 1.2388 - val_acc: 0.7320\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1095 - acc: 0.8073 - val_loss: 1.2361 - val_acc: 0.7360\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1033 - acc: 0.8104 - val_loss: 1.2302 - val_acc: 0.7290\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0971 - acc: 0.8101 - val_loss: 1.2267 - val_acc: 0.7320\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0910 - acc: 0.8113 - val_loss: 1.2216 - val_acc: 0.7350\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0849 - acc: 0.8129 - val_loss: 1.2186 - val_acc: 0.7370\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0789 - acc: 0.8160 - val_loss: 1.2152 - val_acc: 0.7360\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0731 - acc: 0.8185 - val_loss: 1.2099 - val_acc: 0.7400\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0671 - acc: 0.8184 - val_loss: 1.2056 - val_acc: 0.7410\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0616 - acc: 0.8201 - val_loss: 1.2039 - val_acc: 0.7430\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0566 - acc: 0.8208 - val_loss: 1.1997 - val_acc: 0.7430\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0509 - acc: 0.8216 - val_loss: 1.1945 - val_acc: 0.7390\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0450 - acc: 0.8253 - val_loss: 1.1935 - val_acc: 0.7450\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0402 - acc: 0.8255 - val_loss: 1.1864 - val_acc: 0.7480\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0347 - acc: 0.8267 - val_loss: 1.1845 - val_acc: 0.7470\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0296 - acc: 0.8285 - val_loss: 1.1818 - val_acc: 0.7470\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0242 - acc: 0.8288 - val_loss: 1.1785 - val_acc: 0.7460\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0191 - acc: 0.8299 - val_loss: 1.1793 - val_acc: 0.7460\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0144 - acc: 0.8309 - val_loss: 1.1727 - val_acc: 0.7480\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0097 - acc: 0.8331 - val_loss: 1.1713 - val_acc: 0.7470\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0043 - acc: 0.8353 - val_loss: 1.1664 - val_acc: 0.7520\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9994 - acc: 0.8364 - val_loss: 1.1650 - val_acc: 0.7530\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9949 - acc: 0.8399 - val_loss: 1.1598 - val_acc: 0.7510\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9899 - acc: 0.8381 - val_loss: 1.1611 - val_acc: 0.7500\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9855 - acc: 0.8412 - val_loss: 1.1526 - val_acc: 0.7530\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9811 - acc: 0.8421 - val_loss: 1.1533 - val_acc: 0.7560\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9764 - acc: 0.8436 - val_loss: 1.1501 - val_acc: 0.7540\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9718 - acc: 0.8443 - val_loss: 1.1464 - val_acc: 0.7510\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9670 - acc: 0.8445 - val_loss: 1.1434 - val_acc: 0.7560\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9626 - acc: 0.8457 - val_loss: 1.1432 - val_acc: 0.7470\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9585 - acc: 0.8472 - val_loss: 1.1445 - val_acc: 0.7440\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9544 - acc: 0.8475 - val_loss: 1.1373 - val_acc: 0.7510\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9498 - acc: 0.8493 - val_loss: 1.1375 - val_acc: 0.7510\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9454 - acc: 0.8500 - val_loss: 1.1316 - val_acc: 0.7580\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9412 - acc: 0.8515 - val_loss: 1.1298 - val_acc: 0.7560\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9370 - acc: 0.8521 - val_loss: 1.1247 - val_acc: 0.7570\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9332 - acc: 0.8521 - val_loss: 1.1279 - val_acc: 0.7480\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9283 - acc: 0.8540 - val_loss: 1.1230 - val_acc: 0.7540\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9250 - acc: 0.8552 - val_loss: 1.1221 - val_acc: 0.7470\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9213 - acc: 0.8560 - val_loss: 1.1188 - val_acc: 0.7570\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9170 - acc: 0.8548 - val_loss: 1.1148 - val_acc: 0.7550\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9130 - acc: 0.8572 - val_loss: 1.1121 - val_acc: 0.7580\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9088 - acc: 0.8596 - val_loss: 1.1096 - val_acc: 0.7570\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9052 - acc: 0.8584 - val_loss: 1.1112 - val_acc: 0.7550\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9016 - acc: 0.8605 - val_loss: 1.1108 - val_acc: 0.7570\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8976 - acc: 0.8603 - val_loss: 1.1104 - val_acc: 0.7530\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8936 - acc: 0.8627 - val_loss: 1.1036 - val_acc: 0.7550\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8900 - acc: 0.8629 - val_loss: 1.1018 - val_acc: 0.7560\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8866 - acc: 0.8639 - val_loss: 1.1002 - val_acc: 0.7620\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8829 - acc: 0.8648 - val_loss: 1.0963 - val_acc: 0.7630\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8791 - acc: 0.8659 - val_loss: 1.0966 - val_acc: 0.7580\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8756 - acc: 0.8660 - val_loss: 1.0942 - val_acc: 0.7580\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8720 - acc: 0.8663 - val_loss: 1.0912 - val_acc: 0.7600\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8683 - acc: 0.8683 - val_loss: 1.0894 - val_acc: 0.7610\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8649 - acc: 0.8688 - val_loss: 1.0938 - val_acc: 0.7620\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8617 - acc: 0.8689 - val_loss: 1.0883 - val_acc: 0.7620\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8581 - acc: 0.8705 - val_loss: 1.0835 - val_acc: 0.7680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8548 - acc: 0.8733 - val_loss: 1.0847 - val_acc: 0.7640\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, kernel_regularizer=regularizers.l2(0.005),\n",
    "                       activation='relu', input_shape=(2000,)))\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l2(0.005),\n",
    "                       activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. Notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 15.9871 - acc: 0.1704 - val_loss: 15.5785 - val_acc: 0.1800\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 15.2246 - acc: 0.2023 - val_loss: 14.8276 - val_acc: 0.2150\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 14.4854 - acc: 0.2296 - val_loss: 14.0971 - val_acc: 0.2400\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 13.7666 - acc: 0.2487 - val_loss: 13.3876 - val_acc: 0.2730\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 13.0670 - acc: 0.2808 - val_loss: 12.6972 - val_acc: 0.3030\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 12.3861 - acc: 0.3104 - val_loss: 12.0251 - val_acc: 0.3400\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 11.7235 - acc: 0.3408 - val_loss: 11.3715 - val_acc: 0.3630\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 11.0790 - acc: 0.3687 - val_loss: 10.7369 - val_acc: 0.3870\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 10.4536 - acc: 0.3963 - val_loss: 10.1228 - val_acc: 0.4130\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 9.8484 - acc: 0.4279 - val_loss: 9.5288 - val_acc: 0.4470\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 9.2643 - acc: 0.4603 - val_loss: 8.9571 - val_acc: 0.4810\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 8.7027 - acc: 0.4868 - val_loss: 8.4096 - val_acc: 0.5050\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 8.1632 - acc: 0.5207 - val_loss: 7.8839 - val_acc: 0.5370\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 7.6471 - acc: 0.5533 - val_loss: 7.3804 - val_acc: 0.5560\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 7.1540 - acc: 0.5777 - val_loss: 6.9022 - val_acc: 0.5790\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 6.6846 - acc: 0.5937 - val_loss: 6.4470 - val_acc: 0.6040\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 6.2392 - acc: 0.6159 - val_loss: 6.0146 - val_acc: 0.6170\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 5.8180 - acc: 0.6323 - val_loss: 5.6073 - val_acc: 0.6160\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 5.4202 - acc: 0.6428 - val_loss: 5.2216 - val_acc: 0.6360\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 5.0456 - acc: 0.6537 - val_loss: 4.8611 - val_acc: 0.6430\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 4.6938 - acc: 0.6655 - val_loss: 4.5241 - val_acc: 0.6500\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 4.3650 - acc: 0.6671 - val_loss: 4.2065 - val_acc: 0.6590\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 4.0598 - acc: 0.6744 - val_loss: 3.9147 - val_acc: 0.6610\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 3.7774 - acc: 0.6788 - val_loss: 3.6452 - val_acc: 0.6670\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 3.5178 - acc: 0.6799 - val_loss: 3.3983 - val_acc: 0.6750\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 3.2803 - acc: 0.6841 - val_loss: 3.1736 - val_acc: 0.6680\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 3.0647 - acc: 0.6839 - val_loss: 2.9708 - val_acc: 0.6770\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.8703 - acc: 0.6865 - val_loss: 2.7862 - val_acc: 0.6740\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.6966 - acc: 0.6896 - val_loss: 2.6234 - val_acc: 0.6690\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.5430 - acc: 0.6891 - val_loss: 2.4822 - val_acc: 0.6790\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.4102 - acc: 0.6892 - val_loss: 2.3586 - val_acc: 0.6780\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.2972 - acc: 0.6889 - val_loss: 2.2562 - val_acc: 0.6770\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.2032 - acc: 0.6875 - val_loss: 2.1729 - val_acc: 0.6830\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.1277 - acc: 0.6877 - val_loss: 2.1060 - val_acc: 0.6810\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.0691 - acc: 0.6865 - val_loss: 2.0541 - val_acc: 0.6810\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.0262 - acc: 0.6896 - val_loss: 2.0183 - val_acc: 0.6830\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9931 - acc: 0.6891 - val_loss: 1.9904 - val_acc: 0.6870\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9674 - acc: 0.6903 - val_loss: 1.9711 - val_acc: 0.6810\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9450 - acc: 0.6903 - val_loss: 1.9486 - val_acc: 0.6830\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9248 - acc: 0.6888 - val_loss: 1.9270 - val_acc: 0.6830\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9066 - acc: 0.6888 - val_loss: 1.9093 - val_acc: 0.6860\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8894 - acc: 0.6887 - val_loss: 1.8917 - val_acc: 0.6890\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8731 - acc: 0.6912 - val_loss: 1.8783 - val_acc: 0.6900\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8580 - acc: 0.6932 - val_loss: 1.8704 - val_acc: 0.6810\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8435 - acc: 0.6923 - val_loss: 1.8500 - val_acc: 0.6850\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8296 - acc: 0.6920 - val_loss: 1.8377 - val_acc: 0.6880\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8159 - acc: 0.6935 - val_loss: 1.8255 - val_acc: 0.6880\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8036 - acc: 0.6931 - val_loss: 1.8130 - val_acc: 0.6930\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7908 - acc: 0.6951 - val_loss: 1.8006 - val_acc: 0.6980\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.7787 - acc: 0.6967 - val_loss: 1.7909 - val_acc: 0.6850\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.7672 - acc: 0.6981 - val_loss: 1.7794 - val_acc: 0.6950\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7559 - acc: 0.6987 - val_loss: 1.7699 - val_acc: 0.6890\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7446 - acc: 0.6991 - val_loss: 1.7575 - val_acc: 0.6940\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7343 - acc: 0.6988 - val_loss: 1.7459 - val_acc: 0.6930\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7241 - acc: 0.7005 - val_loss: 1.7357 - val_acc: 0.6960\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7134 - acc: 0.7004 - val_loss: 1.7262 - val_acc: 0.6920\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7037 - acc: 0.7007 - val_loss: 1.7162 - val_acc: 0.6930\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6935 - acc: 0.7027 - val_loss: 1.7095 - val_acc: 0.6960\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6845 - acc: 0.7040 - val_loss: 1.6969 - val_acc: 0.6950\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6750 - acc: 0.7039 - val_loss: 1.6875 - val_acc: 0.6950\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6651 - acc: 0.7045 - val_loss: 1.6796 - val_acc: 0.7000\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6564 - acc: 0.7052 - val_loss: 1.6720 - val_acc: 0.6980\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6475 - acc: 0.7063 - val_loss: 1.6623 - val_acc: 0.6980\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6388 - acc: 0.7048 - val_loss: 1.6536 - val_acc: 0.6950\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6301 - acc: 0.7079 - val_loss: 1.6465 - val_acc: 0.6990\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6217 - acc: 0.7075 - val_loss: 1.6376 - val_acc: 0.6980\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6134 - acc: 0.7075 - val_loss: 1.6305 - val_acc: 0.7000\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6059 - acc: 0.7083 - val_loss: 1.6223 - val_acc: 0.6980\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5974 - acc: 0.7101 - val_loss: 1.6161 - val_acc: 0.6970\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5896 - acc: 0.7083 - val_loss: 1.6105 - val_acc: 0.7030\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5818 - acc: 0.7105 - val_loss: 1.6008 - val_acc: 0.6950\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5740 - acc: 0.7108 - val_loss: 1.5916 - val_acc: 0.6930\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5663 - acc: 0.7107 - val_loss: 1.5846 - val_acc: 0.7020\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5590 - acc: 0.7117 - val_loss: 1.5827 - val_acc: 0.6920\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5510 - acc: 0.7101 - val_loss: 1.5740 - val_acc: 0.7040\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5437 - acc: 0.7129 - val_loss: 1.5614 - val_acc: 0.7010\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5366 - acc: 0.7121 - val_loss: 1.5542 - val_acc: 0.7040\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5294 - acc: 0.7140 - val_loss: 1.5489 - val_acc: 0.7030\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5225 - acc: 0.7148 - val_loss: 1.5412 - val_acc: 0.7060\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5149 - acc: 0.7135 - val_loss: 1.5331 - val_acc: 0.7010\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5080 - acc: 0.7136 - val_loss: 1.5291 - val_acc: 0.7010\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5011 - acc: 0.7151 - val_loss: 1.5198 - val_acc: 0.7010\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4942 - acc: 0.7173 - val_loss: 1.5144 - val_acc: 0.6990\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4876 - acc: 0.7153 - val_loss: 1.5081 - val_acc: 0.7060\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4810 - acc: 0.7163 - val_loss: 1.5032 - val_acc: 0.7050\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4744 - acc: 0.7165 - val_loss: 1.5023 - val_acc: 0.7010\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4684 - acc: 0.7179 - val_loss: 1.4907 - val_acc: 0.7040\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4613 - acc: 0.7177 - val_loss: 1.4811 - val_acc: 0.7050\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4551 - acc: 0.7183 - val_loss: 1.4753 - val_acc: 0.7070\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.4488 - acc: 0.7187 - val_loss: 1.4689 - val_acc: 0.7080\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4428 - acc: 0.7185 - val_loss: 1.4633 - val_acc: 0.7050\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4361 - acc: 0.7177 - val_loss: 1.4604 - val_acc: 0.7060\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4303 - acc: 0.7180 - val_loss: 1.4506 - val_acc: 0.7040\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4242 - acc: 0.7185 - val_loss: 1.4461 - val_acc: 0.7030\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4187 - acc: 0.7211 - val_loss: 1.4396 - val_acc: 0.7060\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4123 - acc: 0.7212 - val_loss: 1.4369 - val_acc: 0.7030\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4068 - acc: 0.7232 - val_loss: 1.4299 - val_acc: 0.7020\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4010 - acc: 0.7205 - val_loss: 1.4206 - val_acc: 0.7060\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3950 - acc: 0.7228 - val_loss: 1.4174 - val_acc: 0.7090\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3890 - acc: 0.7219 - val_loss: 1.4116 - val_acc: 0.7090\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3839 - acc: 0.7215 - val_loss: 1.4071 - val_acc: 0.7100\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3777 - acc: 0.7248 - val_loss: 1.3989 - val_acc: 0.7100\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3718 - acc: 0.7241 - val_loss: 1.3945 - val_acc: 0.7110\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3668 - acc: 0.7229 - val_loss: 1.3918 - val_acc: 0.7130\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3616 - acc: 0.7235 - val_loss: 1.3864 - val_acc: 0.7040\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3559 - acc: 0.7247 - val_loss: 1.3825 - val_acc: 0.7160\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3511 - acc: 0.7264 - val_loss: 1.3770 - val_acc: 0.7160\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3460 - acc: 0.7241 - val_loss: 1.3685 - val_acc: 0.7130\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3408 - acc: 0.7253 - val_loss: 1.3647 - val_acc: 0.7130\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3353 - acc: 0.7268 - val_loss: 1.3614 - val_acc: 0.7090\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3309 - acc: 0.7275 - val_loss: 1.3561 - val_acc: 0.7120\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3254 - acc: 0.7251 - val_loss: 1.3496 - val_acc: 0.7160\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3207 - acc: 0.7257 - val_loss: 1.3446 - val_acc: 0.7130\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3156 - acc: 0.7255 - val_loss: 1.3398 - val_acc: 0.7130\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3108 - acc: 0.7283 - val_loss: 1.3369 - val_acc: 0.7170\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3067 - acc: 0.7288 - val_loss: 1.3303 - val_acc: 0.7130\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3019 - acc: 0.7288 - val_loss: 1.3276 - val_acc: 0.7180\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2974 - acc: 0.7267 - val_loss: 1.3218 - val_acc: 0.7160\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2923 - acc: 0.7277 - val_loss: 1.3206 - val_acc: 0.7170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2878 - acc: 0.7285 - val_loss: 1.3143 - val_acc: 0.7160\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, kernel_regularizer=regularizers.l1(0.005),\n",
    "                       activation='relu', input_shape=(2000,)))\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l1(0.005),\n",
    "                       activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r.', label='Validation acc')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the training and validation accuracy don't diverge as much as before. Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like you can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 15.9967 - acc: 0.1321 - val_loss: 15.5787 - val_acc: 0.1610\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 15.2278 - acc: 0.1631 - val_loss: 14.8299 - val_acc: 0.2000\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 14.4924 - acc: 0.1820 - val_loss: 14.1072 - val_acc: 0.2210\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 13.7805 - acc: 0.1981 - val_loss: 13.4059 - val_acc: 0.2350\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 13.0886 - acc: 0.2245 - val_loss: 12.7237 - val_acc: 0.2600\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 12.4152 - acc: 0.2564 - val_loss: 12.0606 - val_acc: 0.2860\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 11.7610 - acc: 0.2843 - val_loss: 11.4172 - val_acc: 0.3060\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 11.1260 - acc: 0.3195 - val_loss: 10.7933 - val_acc: 0.3300\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 10.5099 - acc: 0.3476 - val_loss: 10.1880 - val_acc: 0.3520\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 9.9130 - acc: 0.3760 - val_loss: 9.6039 - val_acc: 0.3690\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 9.3360 - acc: 0.4063 - val_loss: 9.0385 - val_acc: 0.3880\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 8.7795 - acc: 0.4232 - val_loss: 8.4950 - val_acc: 0.4210\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 8.2439 - acc: 0.4479 - val_loss: 7.9704 - val_acc: 0.4310\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 7.7292 - acc: 0.4625 - val_loss: 7.4693 - val_acc: 0.4570\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 7.2363 - acc: 0.4808 - val_loss: 6.9888 - val_acc: 0.4660\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 6.7662 - acc: 0.4921 - val_loss: 6.5325 - val_acc: 0.4710\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 6.3188 - acc: 0.5009 - val_loss: 6.0980 - val_acc: 0.5010\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 5.8944 - acc: 0.5157 - val_loss: 5.6876 - val_acc: 0.5040\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 5.4936 - acc: 0.5263 - val_loss: 5.2999 - val_acc: 0.5050\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 5.1166 - acc: 0.5292 - val_loss: 4.9372 - val_acc: 0.5080\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 4.7635 - acc: 0.5344 - val_loss: 4.5985 - val_acc: 0.5200\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 4.4340 - acc: 0.5407 - val_loss: 4.2821 - val_acc: 0.5260\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 4.1273 - acc: 0.5476 - val_loss: 3.9899 - val_acc: 0.5410\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 3.8438 - acc: 0.5581 - val_loss: 3.7166 - val_acc: 0.5420\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 3.5819 - acc: 0.5659 - val_loss: 3.4705 - val_acc: 0.5460\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 3.3426 - acc: 0.5656 - val_loss: 3.2402 - val_acc: 0.5640\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 3.1244 - acc: 0.5755 - val_loss: 3.0343 - val_acc: 0.5680\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.9284 - acc: 0.5824 - val_loss: 2.8497 - val_acc: 0.5800\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.7544 - acc: 0.5911 - val_loss: 2.6872 - val_acc: 0.5830\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.6011 - acc: 0.5964 - val_loss: 2.5441 - val_acc: 0.5830\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.4687 - acc: 0.5992 - val_loss: 2.4242 - val_acc: 0.5920\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.3570 - acc: 0.6041 - val_loss: 2.3226 - val_acc: 0.6140\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.2644 - acc: 0.6119 - val_loss: 2.2401 - val_acc: 0.6110\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.1909 - acc: 0.6140 - val_loss: 2.1747 - val_acc: 0.6130\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.1337 - acc: 0.6181 - val_loss: 2.1249 - val_acc: 0.6160\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.0915 - acc: 0.6217 - val_loss: 2.0922 - val_acc: 0.6320\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.0604 - acc: 0.6260 - val_loss: 2.0621 - val_acc: 0.6300\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.0355 - acc: 0.6309 - val_loss: 2.0406 - val_acc: 0.6240\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.0140 - acc: 0.6327 - val_loss: 2.0195 - val_acc: 0.6370\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.9937 - acc: 0.6356 - val_loss: 2.0003 - val_acc: 0.6450\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9756 - acc: 0.6411 - val_loss: 1.9848 - val_acc: 0.6420\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.9576 - acc: 0.6432 - val_loss: 1.9656 - val_acc: 0.6430\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9411 - acc: 0.6440 - val_loss: 1.9493 - val_acc: 0.6470\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.9247 - acc: 0.6489 - val_loss: 1.9366 - val_acc: 0.6510\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.9095 - acc: 0.6517 - val_loss: 1.9181 - val_acc: 0.6560\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8942 - acc: 0.6521 - val_loss: 1.9064 - val_acc: 0.6570\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8799 - acc: 0.6556 - val_loss: 1.8907 - val_acc: 0.6580\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8653 - acc: 0.6576 - val_loss: 1.8757 - val_acc: 0.6610\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8519 - acc: 0.6600 - val_loss: 1.8635 - val_acc: 0.6610\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8387 - acc: 0.6628 - val_loss: 1.8526 - val_acc: 0.6590\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8258 - acc: 0.6636 - val_loss: 1.8393 - val_acc: 0.6630\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8135 - acc: 0.6651 - val_loss: 1.8242 - val_acc: 0.6630\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8011 - acc: 0.6665 - val_loss: 1.8134 - val_acc: 0.6660\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7892 - acc: 0.6695 - val_loss: 1.8018 - val_acc: 0.6650\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7777 - acc: 0.6712 - val_loss: 1.7950 - val_acc: 0.6650\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7664 - acc: 0.6727 - val_loss: 1.7788 - val_acc: 0.6680\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.7549 - acc: 0.6752 - val_loss: 1.7686 - val_acc: 0.6680\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7442 - acc: 0.6752 - val_loss: 1.7561 - val_acc: 0.6700\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7335 - acc: 0.6771 - val_loss: 1.7467 - val_acc: 0.6720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7231 - acc: 0.6789 - val_loss: 1.7354 - val_acc: 0.6720\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7130 - acc: 0.6809 - val_loss: 1.7284 - val_acc: 0.6770\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7029 - acc: 0.6797 - val_loss: 1.7175 - val_acc: 0.6720\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6936 - acc: 0.6824 - val_loss: 1.7085 - val_acc: 0.6780\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6838 - acc: 0.6835 - val_loss: 1.6991 - val_acc: 0.6830\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6741 - acc: 0.6851 - val_loss: 1.6886 - val_acc: 0.6840\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6647 - acc: 0.6872 - val_loss: 1.6814 - val_acc: 0.6860\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6559 - acc: 0.6877 - val_loss: 1.6758 - val_acc: 0.6900\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6470 - acc: 0.6880 - val_loss: 1.6631 - val_acc: 0.6870\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6379 - acc: 0.6901 - val_loss: 1.6544 - val_acc: 0.6890\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6293 - acc: 0.6896 - val_loss: 1.6487 - val_acc: 0.6900\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6213 - acc: 0.6899 - val_loss: 1.6371 - val_acc: 0.6870\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6123 - acc: 0.6941 - val_loss: 1.6273 - val_acc: 0.6910\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.6046 - acc: 0.6927 - val_loss: 1.6205 - val_acc: 0.6890\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5966 - acc: 0.6932 - val_loss: 1.6136 - val_acc: 0.6860\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5884 - acc: 0.6945 - val_loss: 1.6077 - val_acc: 0.6940\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5808 - acc: 0.6965 - val_loss: 1.5982 - val_acc: 0.6930\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5731 - acc: 0.6960 - val_loss: 1.5897 - val_acc: 0.6930\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5653 - acc: 0.6973 - val_loss: 1.5835 - val_acc: 0.6950\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.5577 - acc: 0.6985 - val_loss: 1.5750 - val_acc: 0.6950\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.5502 - acc: 0.6991 - val_loss: 1.5690 - val_acc: 0.6960\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5430 - acc: 0.7001 - val_loss: 1.5601 - val_acc: 0.6920\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5360 - acc: 0.7023 - val_loss: 1.5575 - val_acc: 0.6880\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5291 - acc: 0.7027 - val_loss: 1.5468 - val_acc: 0.6940\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5220 - acc: 0.7020 - val_loss: 1.5394 - val_acc: 0.7000\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5153 - acc: 0.7029 - val_loss: 1.5344 - val_acc: 0.6980\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5083 - acc: 0.7056 - val_loss: 1.5260 - val_acc: 0.7010\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.5016 - acc: 0.706 - 0s 33us/step - loss: 1.5014 - acc: 0.7060 - val_loss: 1.5212 - val_acc: 0.6980\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4948 - acc: 0.7061 - val_loss: 1.5155 - val_acc: 0.7020\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4884 - acc: 0.7064 - val_loss: 1.5072 - val_acc: 0.7020\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4816 - acc: 0.7071 - val_loss: 1.5002 - val_acc: 0.6970\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4758 - acc: 0.7067 - val_loss: 1.4964 - val_acc: 0.7000\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4692 - acc: 0.7079 - val_loss: 1.4942 - val_acc: 0.6980\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4631 - acc: 0.7077 - val_loss: 1.4862 - val_acc: 0.7030\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4573 - acc: 0.7079 - val_loss: 1.4782 - val_acc: 0.7040\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4507 - acc: 0.7107 - val_loss: 1.4716 - val_acc: 0.7020\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4445 - acc: 0.7100 - val_loss: 1.4679 - val_acc: 0.7050\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4388 - acc: 0.7104 - val_loss: 1.4607 - val_acc: 0.7040\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4327 - acc: 0.7116 - val_loss: 1.4573 - val_acc: 0.7070\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4269 - acc: 0.7120 - val_loss: 1.4523 - val_acc: 0.7070\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4212 - acc: 0.7111 - val_loss: 1.4415 - val_acc: 0.7080\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4152 - acc: 0.7116 - val_loss: 1.4376 - val_acc: 0.7070\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.4098 - acc: 0.7131 - val_loss: 1.4316 - val_acc: 0.7050\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4039 - acc: 0.7140 - val_loss: 1.4268 - val_acc: 0.7090\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3988 - acc: 0.7132 - val_loss: 1.4195 - val_acc: 0.7090\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3931 - acc: 0.7144 - val_loss: 1.4160 - val_acc: 0.7120\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3878 - acc: 0.7151 - val_loss: 1.4109 - val_acc: 0.7110\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3825 - acc: 0.7161 - val_loss: 1.4048 - val_acc: 0.7100\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3771 - acc: 0.7167 - val_loss: 1.4017 - val_acc: 0.7070\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3717 - acc: 0.7151 - val_loss: 1.3971 - val_acc: 0.7040\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3666 - acc: 0.7167 - val_loss: 1.3920 - val_acc: 0.7150\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3618 - acc: 0.7160 - val_loss: 1.3877 - val_acc: 0.7130\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3562 - acc: 0.7189 - val_loss: 1.3782 - val_acc: 0.7110\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3508 - acc: 0.7157 - val_loss: 1.3762 - val_acc: 0.7130\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3462 - acc: 0.7167 - val_loss: 1.3716 - val_acc: 0.7120\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3405 - acc: 0.7204 - val_loss: 1.3649 - val_acc: 0.7120\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3356 - acc: 0.7204 - val_loss: 1.3606 - val_acc: 0.7110\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3304 - acc: 0.7196 - val_loss: 1.3589 - val_acc: 0.7100\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3260 - acc: 0.7191 - val_loss: 1.3505 - val_acc: 0.7100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3212 - acc: 0.7207 - val_loss: 1.3465 - val_acc: 0.7120\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3165 - acc: 0.7209 - val_loss: 1.3394 - val_acc: 0.7170\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3115 - acc: 0.7200 - val_loss: 1.3380 - val_acc: 0.7150\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3068 - acc: 0.7223 - val_loss: 1.3334 - val_acc: 0.7140\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3027 - acc: 0.7196 - val_loss: 1.3357 - val_acc: 0.7110\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2981 - acc: 0.7217 - val_loss: 1.3261 - val_acc: 0.7140\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2944 - acc: 0.7223 - val_loss: 1.3188 - val_acc: 0.7170\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.2891 - acc: 0.723 - 0s 35us/step - loss: 1.2888 - acc: 0.7237 - val_loss: 1.3140 - val_acc: 0.7110\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2844 - acc: 0.7241 - val_loss: 1.3135 - val_acc: 0.7150\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2799 - acc: 0.7247 - val_loss: 1.3074 - val_acc: 0.7100\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2756 - acc: 0.7244 - val_loss: 1.3060 - val_acc: 0.7170\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2719 - acc: 0.7261 - val_loss: 1.3060 - val_acc: 0.7110\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2673 - acc: 0.7257 - val_loss: 1.2946 - val_acc: 0.7160\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2634 - acc: 0.7275 - val_loss: 1.2913 - val_acc: 0.7190\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2589 - acc: 0.7271 - val_loss: 1.2875 - val_acc: 0.7190\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2548 - acc: 0.7257 - val_loss: 1.2878 - val_acc: 0.7150\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2508 - acc: 0.7267 - val_loss: 1.2867 - val_acc: 0.7160\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2467 - acc: 0.7289 - val_loss: 1.2761 - val_acc: 0.7180\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2427 - acc: 0.7280 - val_loss: 1.2703 - val_acc: 0.7160\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2388 - acc: 0.7287 - val_loss: 1.2665 - val_acc: 0.7170\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2349 - acc: 0.7288 - val_loss: 1.2650 - val_acc: 0.7150\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2302 - acc: 0.7299 - val_loss: 1.2622 - val_acc: 0.7170\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2265 - acc: 0.7296 - val_loss: 1.2590 - val_acc: 0.7180\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2230 - acc: 0.7328 - val_loss: 1.2554 - val_acc: 0.7240\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2191 - acc: 0.7312 - val_loss: 1.2521 - val_acc: 0.7210\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2156 - acc: 0.7329 - val_loss: 1.2503 - val_acc: 0.7260\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2120 - acc: 0.7307 - val_loss: 1.2488 - val_acc: 0.7110\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2089 - acc: 0.7311 - val_loss: 1.2402 - val_acc: 0.7180\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2048 - acc: 0.7325 - val_loss: 1.2382 - val_acc: 0.7210\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2013 - acc: 0.7353 - val_loss: 1.2364 - val_acc: 0.7150\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1979 - acc: 0.7327 - val_loss: 1.2310 - val_acc: 0.7170\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1947 - acc: 0.7345 - val_loss: 1.2292 - val_acc: 0.7230\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1911 - acc: 0.7352 - val_loss: 1.2250 - val_acc: 0.7200\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1885 - acc: 0.7343 - val_loss: 1.2228 - val_acc: 0.7170\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1848 - acc: 0.7348 - val_loss: 1.2217 - val_acc: 0.7220\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1815 - acc: 0.7348 - val_loss: 1.2167 - val_acc: 0.7180\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1778 - acc: 0.7357 - val_loss: 1.2128 - val_acc: 0.7220\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1751 - acc: 0.7360 - val_loss: 1.2111 - val_acc: 0.7160\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1717 - acc: 0.7369 - val_loss: 1.2072 - val_acc: 0.7180\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1690 - acc: 0.7361 - val_loss: 1.2042 - val_acc: 0.7230\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1651 - acc: 0.7363 - val_loss: 1.2085 - val_acc: 0.7190\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1626 - acc: 0.7372 - val_loss: 1.2001 - val_acc: 0.7270\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1599 - acc: 0.7380 - val_loss: 1.1956 - val_acc: 0.7270\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1570 - acc: 0.7363 - val_loss: 1.1967 - val_acc: 0.7240\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1542 - acc: 0.7389 - val_loss: 1.1936 - val_acc: 0.7240\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1508 - acc: 0.7383 - val_loss: 1.1889 - val_acc: 0.7220\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1480 - acc: 0.7407 - val_loss: 1.1855 - val_acc: 0.7270\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1452 - acc: 0.7381 - val_loss: 1.1809 - val_acc: 0.7260\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1418 - acc: 0.7413 - val_loss: 1.1791 - val_acc: 0.7240\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1394 - acc: 0.7396 - val_loss: 1.1803 - val_acc: 0.7240\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1369 - acc: 0.7407 - val_loss: 1.1807 - val_acc: 0.7230\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1350 - acc: 0.7391 - val_loss: 1.1720 - val_acc: 0.7260\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1318 - acc: 0.7420 - val_loss: 1.1709 - val_acc: 0.7260\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1289 - acc: 0.7409 - val_loss: 1.1694 - val_acc: 0.7250\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1264 - acc: 0.7408 - val_loss: 1.1656 - val_acc: 0.7230\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1239 - acc: 0.7440 - val_loss: 1.1665 - val_acc: 0.7240\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1215 - acc: 0.7421 - val_loss: 1.1610 - val_acc: 0.7230\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1187 - acc: 0.7436 - val_loss: 1.1611 - val_acc: 0.7230\n",
      "Epoch 177/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1170 - acc: 0.7413 - val_loss: 1.1586 - val_acc: 0.7230\n",
      "Epoch 178/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1149 - acc: 0.7436 - val_loss: 1.1536 - val_acc: 0.7240\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1127 - acc: 0.7436 - val_loss: 1.1516 - val_acc: 0.7270\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1097 - acc: 0.7424 - val_loss: 1.1512 - val_acc: 0.7280\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1077 - acc: 0.7431 - val_loss: 1.1495 - val_acc: 0.7280\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1061 - acc: 0.7456 - val_loss: 1.1518 - val_acc: 0.7300\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1038 - acc: 0.7443 - val_loss: 1.1447 - val_acc: 0.7300\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1014 - acc: 0.7435 - val_loss: 1.1410 - val_acc: 0.7250\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0999 - acc: 0.7461 - val_loss: 1.1452 - val_acc: 0.7250\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0976 - acc: 0.7443 - val_loss: 1.1429 - val_acc: 0.7260\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0950 - acc: 0.7463 - val_loss: 1.1419 - val_acc: 0.7240\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0936 - acc: 0.7451 - val_loss: 1.1427 - val_acc: 0.7280\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0921 - acc: 0.7452 - val_loss: 1.1381 - val_acc: 0.7270\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0901 - acc: 0.7480 - val_loss: 1.1372 - val_acc: 0.7290\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0881 - acc: 0.7471 - val_loss: 1.1327 - val_acc: 0.7250\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0860 - acc: 0.7491 - val_loss: 1.1293 - val_acc: 0.7260\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0842 - acc: 0.7472 - val_loss: 1.1303 - val_acc: 0.7260\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0820 - acc: 0.7471 - val_loss: 1.1419 - val_acc: 0.7210\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0822 - acc: 0.7479 - val_loss: 1.1265 - val_acc: 0.7340\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0793 - acc: 0.7485 - val_loss: 1.1265 - val_acc: 0.7290\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0776 - acc: 0.7468 - val_loss: 1.1242 - val_acc: 0.7280\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0762 - acc: 0.7491 - val_loss: 1.1228 - val_acc: 0.7300\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0744 - acc: 0.7473 - val_loss: 1.1189 - val_acc: 0.7280\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0725 - acc: 0.7476 - val_loss: 1.1172 - val_acc: 0.7290\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0708 - acc: 0.7508 - val_loss: 1.1167 - val_acc: 0.7280\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0695 - acc: 0.7500 - val_loss: 1.1158 - val_acc: 0.7330\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0680 - acc: 0.7501 - val_loss: 1.1178 - val_acc: 0.7240\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0668 - acc: 0.7499 - val_loss: 1.1143 - val_acc: 0.7330\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0649 - acc: 0.7504 - val_loss: 1.1091 - val_acc: 0.7320\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0633 - acc: 0.7508 - val_loss: 1.1068 - val_acc: 0.7310\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0620 - acc: 0.7504 - val_loss: 1.1101 - val_acc: 0.7310\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0602 - acc: 0.7505 - val_loss: 1.1088 - val_acc: 0.7280\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0592 - acc: 0.7515 - val_loss: 1.1116 - val_acc: 0.7240\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0578 - acc: 0.7508 - val_loss: 1.1035 - val_acc: 0.7330\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0561 - acc: 0.7524 - val_loss: 1.1029 - val_acc: 0.7350\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0542 - acc: 0.7523 - val_loss: 1.1001 - val_acc: 0.7330\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0539 - acc: 0.7527 - val_loss: 1.1128 - val_acc: 0.7240\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0527 - acc: 0.7537 - val_loss: 1.1033 - val_acc: 0.7290\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0505 - acc: 0.7521 - val_loss: 1.0972 - val_acc: 0.7340\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0496 - acc: 0.7528 - val_loss: 1.0964 - val_acc: 0.7350\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0480 - acc: 0.7481 - val_loss: 1.0963 - val_acc: 0.7330\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0467 - acc: 0.7508 - val_loss: 1.0938 - val_acc: 0.7330\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0449 - acc: 0.7523 - val_loss: 1.0947 - val_acc: 0.7310\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0442 - acc: 0.7541 - val_loss: 1.0913 - val_acc: 0.7310\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0427 - acc: 0.7516 - val_loss: 1.1094 - val_acc: 0.7220\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0430 - acc: 0.7511 - val_loss: 1.0883 - val_acc: 0.7320\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0409 - acc: 0.7523 - val_loss: 1.0886 - val_acc: 0.7330\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0384 - acc: 0.7583 - val_loss: 1.0908 - val_acc: 0.7280\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0380 - acc: 0.7541 - val_loss: 1.0846 - val_acc: 0.7360\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0364 - acc: 0.7551 - val_loss: 1.0854 - val_acc: 0.7380\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0356 - acc: 0.7553 - val_loss: 1.0823 - val_acc: 0.7410\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0342 - acc: 0.7556 - val_loss: 1.0850 - val_acc: 0.7360\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0331 - acc: 0.7545 - val_loss: 1.0858 - val_acc: 0.7280\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0323 - acc: 0.7536 - val_loss: 1.0871 - val_acc: 0.7260\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0315 - acc: 0.7555 - val_loss: 1.0823 - val_acc: 0.7340\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0312 - acc: 0.7551 - val_loss: 1.0824 - val_acc: 0.7320\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0294 - acc: 0.7533 - val_loss: 1.0851 - val_acc: 0.7280\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0284 - acc: 0.7557 - val_loss: 1.0777 - val_acc: 0.7300\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0268 - acc: 0.7552 - val_loss: 1.0817 - val_acc: 0.7290\n",
      "Epoch 236/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0260 - acc: 0.7555 - val_loss: 1.0794 - val_acc: 0.7320\n",
      "Epoch 237/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0256 - acc: 0.7568 - val_loss: 1.0750 - val_acc: 0.7340\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0245 - acc: 0.7567 - val_loss: 1.0828 - val_acc: 0.7290\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0241 - acc: 0.7556 - val_loss: 1.0738 - val_acc: 0.7350\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0218 - acc: 0.7555 - val_loss: 1.0737 - val_acc: 0.7310\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0212 - acc: 0.7561 - val_loss: 1.0750 - val_acc: 0.7360\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0207 - acc: 0.7568 - val_loss: 1.0713 - val_acc: 0.7350\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0193 - acc: 0.7563 - val_loss: 1.0750 - val_acc: 0.7300\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0179 - acc: 0.7556 - val_loss: 1.0728 - val_acc: 0.7290\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0179 - acc: 0.7592 - val_loss: 1.0698 - val_acc: 0.7280\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0162 - acc: 0.7569 - val_loss: 1.0678 - val_acc: 0.7340\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0157 - acc: 0.7568 - val_loss: 1.0680 - val_acc: 0.7390\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0156 - acc: 0.7561 - val_loss: 1.0688 - val_acc: 0.7340\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0135 - acc: 0.7601 - val_loss: 1.0705 - val_acc: 0.7430\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0134 - acc: 0.7565 - val_loss: 1.0624 - val_acc: 0.7420\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0116 - acc: 0.7580 - val_loss: 1.0688 - val_acc: 0.7300\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0110 - acc: 0.7577 - val_loss: 1.0623 - val_acc: 0.7340\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0100 - acc: 0.7599 - val_loss: 1.0643 - val_acc: 0.7360\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0102 - acc: 0.7575 - val_loss: 1.0630 - val_acc: 0.7330\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0081 - acc: 0.7605 - val_loss: 1.0663 - val_acc: 0.7320\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0073 - acc: 0.7587 - val_loss: 1.0594 - val_acc: 0.7370\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0064 - acc: 0.7601 - val_loss: 1.0599 - val_acc: 0.7350\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0055 - acc: 0.7569 - val_loss: 1.0595 - val_acc: 0.7310\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0050 - acc: 0.7612 - val_loss: 1.0569 - val_acc: 0.7400\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0043 - acc: 0.7592 - val_loss: 1.0559 - val_acc: 0.7360\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0033 - acc: 0.7592 - val_loss: 1.0583 - val_acc: 0.7360\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0024 - acc: 0.7593 - val_loss: 1.0548 - val_acc: 0.7360\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0018 - acc: 0.7592 - val_loss: 1.0554 - val_acc: 0.7370\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0006 - acc: 0.7589 - val_loss: 1.0523 - val_acc: 0.7380\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0007 - acc: 0.7597 - val_loss: 1.0634 - val_acc: 0.7260\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9998 - acc: 0.7639 - val_loss: 1.0562 - val_acc: 0.7330\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9989 - acc: 0.7616 - val_loss: 1.0550 - val_acc: 0.7370\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9988 - acc: 0.7605 - val_loss: 1.0502 - val_acc: 0.7370\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9968 - acc: 0.7589 - val_loss: 1.0507 - val_acc: 0.7360\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9972 - acc: 0.7596 - val_loss: 1.0538 - val_acc: 0.7350\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9963 - acc: 0.7617 - val_loss: 1.0513 - val_acc: 0.7370\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9951 - acc: 0.7612 - val_loss: 1.0505 - val_acc: 0.7410\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9946 - acc: 0.7604 - val_loss: 1.0459 - val_acc: 0.7360\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9937 - acc: 0.7631 - val_loss: 1.0504 - val_acc: 0.7340\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9930 - acc: 0.7617 - val_loss: 1.0455 - val_acc: 0.7370\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9919 - acc: 0.7633 - val_loss: 1.0489 - val_acc: 0.7300\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9911 - acc: 0.7613 - val_loss: 1.0463 - val_acc: 0.7380\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9914 - acc: 0.7587 - val_loss: 1.0492 - val_acc: 0.7390\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9898 - acc: 0.7605 - val_loss: 1.0432 - val_acc: 0.7370\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9888 - acc: 0.7623 - val_loss: 1.0437 - val_acc: 0.7370\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9888 - acc: 0.7624 - val_loss: 1.0439 - val_acc: 0.7370\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9883 - acc: 0.7625 - val_loss: 1.0431 - val_acc: 0.7340\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9874 - acc: 0.7635 - val_loss: 1.0419 - val_acc: 0.7360\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9861 - acc: 0.7613 - val_loss: 1.0456 - val_acc: 0.7380\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9853 - acc: 0.7629 - val_loss: 1.0418 - val_acc: 0.7380\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9852 - acc: 0.7615 - val_loss: 1.0408 - val_acc: 0.7320\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9841 - acc: 0.7627 - val_loss: 1.0415 - val_acc: 0.7350\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9832 - acc: 0.7641 - val_loss: 1.0386 - val_acc: 0.7370\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9832 - acc: 0.7621 - val_loss: 1.0420 - val_acc: 0.7390\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9814 - acc: 0.7623 - val_loss: 1.0376 - val_acc: 0.7370\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9814 - acc: 0.7648 - val_loss: 1.0527 - val_acc: 0.7310\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9817 - acc: 0.7617 - val_loss: 1.0617 - val_acc: 0.7180\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9814 - acc: 0.7636 - val_loss: 1.0392 - val_acc: 0.7330\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9808 - acc: 0.7625 - val_loss: 1.0407 - val_acc: 0.7390\n",
      "Epoch 295/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9793 - acc: 0.7671 - val_loss: 1.0328 - val_acc: 0.7380\n",
      "Epoch 296/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9781 - acc: 0.7631 - val_loss: 1.0359 - val_acc: 0.7330\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9777 - acc: 0.7643 - val_loss: 1.0578 - val_acc: 0.7280\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9786 - acc: 0.7640 - val_loss: 1.0349 - val_acc: 0.7400\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9763 - acc: 0.7639 - val_loss: 1.0352 - val_acc: 0.7430\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9756 - acc: 0.7657 - val_loss: 1.0383 - val_acc: 0.7360\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9752 - acc: 0.7639 - val_loss: 1.0363 - val_acc: 0.7360\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9747 - acc: 0.7652 - val_loss: 1.0355 - val_acc: 0.7420\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9734 - acc: 0.7632 - val_loss: 1.0322 - val_acc: 0.7320\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9734 - acc: 0.7633 - val_loss: 1.0360 - val_acc: 0.7340\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9734 - acc: 0.7653 - val_loss: 1.0320 - val_acc: 0.7390\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9730 - acc: 0.7635 - val_loss: 1.0313 - val_acc: 0.7370\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9720 - acc: 0.7657 - val_loss: 1.0301 - val_acc: 0.7390\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9705 - acc: 0.7648 - val_loss: 1.0291 - val_acc: 0.7410\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9711 - acc: 0.7643 - val_loss: 1.0301 - val_acc: 0.7350\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9703 - acc: 0.7657 - val_loss: 1.0295 - val_acc: 0.7420\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9692 - acc: 0.7652 - val_loss: 1.0280 - val_acc: 0.7430\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9681 - acc: 0.7651 - val_loss: 1.0312 - val_acc: 0.7360\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9680 - acc: 0.7672 - val_loss: 1.0332 - val_acc: 0.7310\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9674 - acc: 0.7661 - val_loss: 1.0391 - val_acc: 0.7380\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9670 - acc: 0.7635 - val_loss: 1.0276 - val_acc: 0.7310\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9674 - acc: 0.7672 - val_loss: 1.0260 - val_acc: 0.7340\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9660 - acc: 0.7661 - val_loss: 1.0230 - val_acc: 0.7400\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9656 - acc: 0.7671 - val_loss: 1.0251 - val_acc: 0.7400\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9645 - acc: 0.7651 - val_loss: 1.0264 - val_acc: 0.7360\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9638 - acc: 0.7644 - val_loss: 1.0315 - val_acc: 0.7380\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9639 - acc: 0.7652 - val_loss: 1.0234 - val_acc: 0.7370\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9639 - acc: 0.7613 - val_loss: 1.0226 - val_acc: 0.7360\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9619 - acc: 0.7688 - val_loss: 1.0241 - val_acc: 0.7340\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9617 - acc: 0.7672 - val_loss: 1.0233 - val_acc: 0.7360\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9611 - acc: 0.7684 - val_loss: 1.0220 - val_acc: 0.7430\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9613 - acc: 0.7653 - val_loss: 1.0209 - val_acc: 0.7380\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9604 - acc: 0.7669 - val_loss: 1.0278 - val_acc: 0.7370\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9604 - acc: 0.7663 - val_loss: 1.0225 - val_acc: 0.7340\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9604 - acc: 0.7676 - val_loss: 1.0188 - val_acc: 0.7390\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9585 - acc: 0.7673 - val_loss: 1.0176 - val_acc: 0.7380\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9572 - acc: 0.7672 - val_loss: 1.0240 - val_acc: 0.7350\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9567 - acc: 0.7665 - val_loss: 1.0219 - val_acc: 0.7360\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9570 - acc: 0.7708 - val_loss: 1.0158 - val_acc: 0.7380\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9562 - acc: 0.7687 - val_loss: 1.0220 - val_acc: 0.7410\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9552 - acc: 0.7699 - val_loss: 1.0188 - val_acc: 0.7360\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9559 - acc: 0.7671 - val_loss: 1.0141 - val_acc: 0.7370\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9558 - acc: 0.7660 - val_loss: 1.0159 - val_acc: 0.7470\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9544 - acc: 0.7671 - val_loss: 1.0280 - val_acc: 0.7330\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9538 - acc: 0.7664 - val_loss: 1.0144 - val_acc: 0.7360\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9536 - acc: 0.7671 - val_loss: 1.0185 - val_acc: 0.7440\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9529 - acc: 0.7687 - val_loss: 1.0168 - val_acc: 0.7360\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9523 - acc: 0.7657 - val_loss: 1.0111 - val_acc: 0.7390\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9513 - acc: 0.7683 - val_loss: 1.0124 - val_acc: 0.7390\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9509 - acc: 0.7685 - val_loss: 1.0128 - val_acc: 0.7430\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9494 - acc: 0.7701 - val_loss: 1.0215 - val_acc: 0.7330\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9503 - acc: 0.7679 - val_loss: 1.0128 - val_acc: 0.7350\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9491 - acc: 0.7697 - val_loss: 1.0134 - val_acc: 0.7360\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9491 - acc: 0.7696 - val_loss: 1.0127 - val_acc: 0.7420\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9486 - acc: 0.7688 - val_loss: 1.0104 - val_acc: 0.7440\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9475 - acc: 0.7667 - val_loss: 1.0099 - val_acc: 0.7480\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9471 - acc: 0.7684 - val_loss: 1.0069 - val_acc: 0.7400\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9471 - acc: 0.7676 - val_loss: 1.0129 - val_acc: 0.7350\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9468 - acc: 0.7685 - val_loss: 1.0084 - val_acc: 0.7390\n",
      "Epoch 354/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9453 - acc: 0.7712 - val_loss: 1.0220 - val_acc: 0.7320\n",
      "Epoch 355/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9462 - acc: 0.7693 - val_loss: 1.0084 - val_acc: 0.7400\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9452 - acc: 0.7689 - val_loss: 1.0080 - val_acc: 0.7450\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9442 - acc: 0.7679 - val_loss: 1.0058 - val_acc: 0.7440\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9441 - acc: 0.7707 - val_loss: 1.0050 - val_acc: 0.7340\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9431 - acc: 0.7691 - val_loss: 1.0105 - val_acc: 0.7340\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9428 - acc: 0.7705 - val_loss: 1.0098 - val_acc: 0.7340\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9426 - acc: 0.7688 - val_loss: 1.0046 - val_acc: 0.7410\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9427 - acc: 0.7688 - val_loss: 1.0087 - val_acc: 0.7410\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9414 - acc: 0.7731 - val_loss: 1.0101 - val_acc: 0.7400\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9416 - acc: 0.7711 - val_loss: 1.0035 - val_acc: 0.7350\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9414 - acc: 0.7689 - val_loss: 1.0129 - val_acc: 0.7410\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9415 - acc: 0.769 - 0s 35us/step - loss: 0.9405 - acc: 0.7692 - val_loss: 1.0077 - val_acc: 0.7460\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9397 - acc: 0.7688 - val_loss: 1.0062 - val_acc: 0.7390\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9393 - acc: 0.7695 - val_loss: 1.0112 - val_acc: 0.7400\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9408 - acc: 0.7684 - val_loss: 1.0056 - val_acc: 0.7330\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9390 - acc: 0.7701 - val_loss: 1.0078 - val_acc: 0.7400\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9400 - acc: 0.7683 - val_loss: 1.0065 - val_acc: 0.7300\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9385 - acc: 0.7721 - val_loss: 1.0057 - val_acc: 0.7380\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9370 - acc: 0.7707 - val_loss: 1.0093 - val_acc: 0.7360\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9362 - acc: 0.7731 - val_loss: 1.0006 - val_acc: 0.7380\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9362 - acc: 0.7711 - val_loss: 1.0001 - val_acc: 0.7370\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9358 - acc: 0.7708 - val_loss: 1.0012 - val_acc: 0.7450\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9357 - acc: 0.7700 - val_loss: 1.0035 - val_acc: 0.7390\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9359 - acc: 0.7708 - val_loss: 1.0008 - val_acc: 0.7370\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9340 - acc: 0.7707 - val_loss: 1.0053 - val_acc: 0.7390\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9355 - acc: 0.7721 - val_loss: 1.0107 - val_acc: 0.7280\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9353 - acc: 0.7712 - val_loss: 0.9996 - val_acc: 0.7440\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9334 - acc: 0.7723 - val_loss: 1.0074 - val_acc: 0.7390\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9331 - acc: 0.7725 - val_loss: 1.0074 - val_acc: 0.7380\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9326 - acc: 0.7740 - val_loss: 0.9980 - val_acc: 0.7390\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9324 - acc: 0.7707 - val_loss: 1.0045 - val_acc: 0.7300\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9323 - acc: 0.7699 - val_loss: 0.9970 - val_acc: 0.7340\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9312 - acc: 0.7739 - val_loss: 0.9999 - val_acc: 0.7370\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9307 - acc: 0.7709 - val_loss: 0.9953 - val_acc: 0.7350\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9295 - acc: 0.7737 - val_loss: 0.9968 - val_acc: 0.7410\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9306 - acc: 0.7731 - val_loss: 0.9977 - val_acc: 0.7400\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9310 - acc: 0.7724 - val_loss: 0.9944 - val_acc: 0.7350\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9296 - acc: 0.7729 - val_loss: 0.9976 - val_acc: 0.7390\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9288 - acc: 0.7753 - val_loss: 0.9982 - val_acc: 0.7380\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9293 - acc: 0.7727 - val_loss: 0.9980 - val_acc: 0.7410\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9276 - acc: 0.7728 - val_loss: 0.9941 - val_acc: 0.7380\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9289 - acc: 0.7725 - val_loss: 1.0015 - val_acc: 0.7350\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9283 - acc: 0.7719 - val_loss: 1.0032 - val_acc: 0.7300\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9277 - acc: 0.7745 - val_loss: 0.9923 - val_acc: 0.7410\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9270 - acc: 0.7735 - val_loss: 0.9969 - val_acc: 0.7380\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9265 - acc: 0.7755 - val_loss: 1.0087 - val_acc: 0.7370\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9260 - acc: 0.7749 - val_loss: 1.0068 - val_acc: 0.7350\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9270 - acc: 0.7692 - val_loss: 0.9893 - val_acc: 0.7400\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9252 - acc: 0.7719 - val_loss: 0.9974 - val_acc: 0.7440\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9251 - acc: 0.7751 - val_loss: 1.0044 - val_acc: 0.7380\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9259 - acc: 0.7751 - val_loss: 0.9954 - val_acc: 0.7330\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9241 - acc: 0.7744 - val_loss: 0.9985 - val_acc: 0.7400\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9237 - acc: 0.7757 - val_loss: 0.9889 - val_acc: 0.7360\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9235 - acc: 0.7727 - val_loss: 0.9942 - val_acc: 0.7420\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9228 - acc: 0.7748 - val_loss: 1.0282 - val_acc: 0.7170\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9248 - acc: 0.7736 - val_loss: 1.0033 - val_acc: 0.7340\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9251 - acc: 0.7725 - val_loss: 1.0033 - val_acc: 0.7390\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9221 - acc: 0.7737 - val_loss: 0.9931 - val_acc: 0.7400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9220 - acc: 0.7727 - val_loss: 0.9884 - val_acc: 0.7370\n",
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9223 - acc: 0.7725 - val_loss: 0.9887 - val_acc: 0.7420\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9202 - acc: 0.7765 - val_loss: 0.9922 - val_acc: 0.7350\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9210 - acc: 0.7753 - val_loss: 0.9911 - val_acc: 0.7440\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9201 - acc: 0.7733 - val_loss: 0.9843 - val_acc: 0.7430\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9199 - acc: 0.7773 - val_loss: 0.9866 - val_acc: 0.7460\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9200 - acc: 0.7769 - val_loss: 0.9900 - val_acc: 0.7450\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9184 - acc: 0.7751 - val_loss: 0.9827 - val_acc: 0.7430\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9180 - acc: 0.7735 - val_loss: 1.0198 - val_acc: 0.7220\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9193 - acc: 0.7747 - val_loss: 0.9928 - val_acc: 0.7380\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9195 - acc: 0.7772 - val_loss: 0.9951 - val_acc: 0.7370\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9178 - acc: 0.7781 - val_loss: 1.0014 - val_acc: 0.7330\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9176 - acc: 0.7765 - val_loss: 0.9869 - val_acc: 0.7370\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9165 - acc: 0.7756 - val_loss: 1.0016 - val_acc: 0.7260\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9168 - acc: 0.7732 - val_loss: 0.9858 - val_acc: 0.7380\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9163 - acc: 0.7755 - val_loss: 0.9901 - val_acc: 0.7430\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9164 - acc: 0.7759 - val_loss: 0.9940 - val_acc: 0.7390\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9155 - acc: 0.7767 - val_loss: 0.9933 - val_acc: 0.7430\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9161 - acc: 0.7753 - val_loss: 0.9871 - val_acc: 0.7360\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9162 - acc: 0.7787 - val_loss: 0.9846 - val_acc: 0.7460\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9155 - acc: 0.7752 - val_loss: 0.9861 - val_acc: 0.7430\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9148 - acc: 0.7775 - val_loss: 0.9922 - val_acc: 0.7310\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9139 - acc: 0.7799 - val_loss: 0.9824 - val_acc: 0.7420\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9133 - acc: 0.7783 - val_loss: 1.0031 - val_acc: 0.7220\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9164 - acc: 0.7767 - val_loss: 0.9844 - val_acc: 0.7460\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9135 - acc: 0.7783 - val_loss: 0.9861 - val_acc: 0.7420\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9131 - acc: 0.7785 - val_loss: 0.9923 - val_acc: 0.7390\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9123 - acc: 0.7772 - val_loss: 0.9800 - val_acc: 0.7460\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9112 - acc: 0.7773 - val_loss: 0.9816 - val_acc: 0.7450\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9110 - acc: 0.7777 - val_loss: 0.9818 - val_acc: 0.7390\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9112 - acc: 0.7781 - val_loss: 0.9792 - val_acc: 0.7390\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9106 - acc: 0.7787 - val_loss: 0.9966 - val_acc: 0.7390\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9124 - acc: 0.7793 - val_loss: 1.0093 - val_acc: 0.7340\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9111 - acc: 0.7781 - val_loss: 0.9823 - val_acc: 0.7390\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9099 - acc: 0.7788 - val_loss: 0.9820 - val_acc: 0.7470\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9094 - acc: 0.7803 - val_loss: 0.9785 - val_acc: 0.7400\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9101 - acc: 0.7808 - val_loss: 0.9827 - val_acc: 0.7450\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9089 - acc: 0.7799 - val_loss: 0.9815 - val_acc: 0.7400\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9085 - acc: 0.7793 - val_loss: 0.9820 - val_acc: 0.7440\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9086 - acc: 0.7788 - val_loss: 0.9833 - val_acc: 0.7410\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9087 - acc: 0.7800 - val_loss: 0.9777 - val_acc: 0.7440\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9090 - acc: 0.7783 - val_loss: 0.9809 - val_acc: 0.7430\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9094 - acc: 0.7791 - val_loss: 0.9842 - val_acc: 0.7410\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9057 - acc: 0.7793 - val_loss: 0.9808 - val_acc: 0.7370\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9076 - acc: 0.7796 - val_loss: 0.9788 - val_acc: 0.7400\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9073 - acc: 0.7795 - val_loss: 0.9806 - val_acc: 0.7400\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9084 - acc: 0.7789 - val_loss: 0.9994 - val_acc: 0.7310\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9075 - acc: 0.7785 - val_loss: 0.9795 - val_acc: 0.7400\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9064 - acc: 0.7796 - val_loss: 0.9765 - val_acc: 0.7460\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9054 - acc: 0.7788 - val_loss: 0.9756 - val_acc: 0.7460\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9053 - acc: 0.7785 - val_loss: 0.9736 - val_acc: 0.7440\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9067 - acc: 0.7799 - val_loss: 0.9913 - val_acc: 0.7410\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9063 - acc: 0.7789 - val_loss: 0.9858 - val_acc: 0.7320\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9044 - acc: 0.7820 - val_loss: 0.9760 - val_acc: 0.7450\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9048 - acc: 0.7791 - val_loss: 0.9918 - val_acc: 0.7270\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9046 - acc: 0.7825 - val_loss: 0.9727 - val_acc: 0.7370\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9035 - acc: 0.7812 - val_loss: 0.9774 - val_acc: 0.7450\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9035 - acc: 0.7819 - val_loss: 0.9754 - val_acc: 0.7430\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9032 - acc: 0.7832 - val_loss: 0.9742 - val_acc: 0.7440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9034 - acc: 0.7796 - val_loss: 0.9790 - val_acc: 0.7340\n",
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9037 - acc: 0.7799 - val_loss: 0.9735 - val_acc: 0.7410\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9032 - acc: 0.7843 - val_loss: 0.9825 - val_acc: 0.7390\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9033 - acc: 0.7816 - val_loss: 0.9733 - val_acc: 0.7390\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9037 - acc: 0.7823 - val_loss: 0.9770 - val_acc: 0.7510\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9015 - acc: 0.7817 - val_loss: 0.9790 - val_acc: 0.7480\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9008 - acc: 0.7828 - val_loss: 0.9830 - val_acc: 0.7480\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9007 - acc: 0.7807 - val_loss: 0.9749 - val_acc: 0.7470\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9013 - acc: 0.7816 - val_loss: 1.0214 - val_acc: 0.7160\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9031 - acc: 0.7821 - val_loss: 0.9778 - val_acc: 0.7450\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8997 - acc: 0.7829 - val_loss: 0.9809 - val_acc: 0.7410\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9016 - acc: 0.7833 - val_loss: 0.9771 - val_acc: 0.7390\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9018 - acc: 0.7836 - val_loss: 0.9735 - val_acc: 0.7430\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9000 - acc: 0.7831 - val_loss: 0.9759 - val_acc: 0.7420\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8998 - acc: 0.7849 - val_loss: 0.9702 - val_acc: 0.7460\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9007 - acc: 0.7835 - val_loss: 0.9770 - val_acc: 0.7450\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8998 - acc: 0.7808 - val_loss: 0.9814 - val_acc: 0.7500\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8987 - acc: 0.7824 - val_loss: 0.9716 - val_acc: 0.7440\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8979 - acc: 0.7816 - val_loss: 0.9774 - val_acc: 0.7420\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8997 - acc: 0.7833 - val_loss: 0.9766 - val_acc: 0.7390\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8988 - acc: 0.7824 - val_loss: 0.9898 - val_acc: 0.7330\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8983 - acc: 0.7851 - val_loss: 0.9755 - val_acc: 0.7340\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8979 - acc: 0.7827 - val_loss: 0.9900 - val_acc: 0.7330\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8972 - acc: 0.7837 - val_loss: 0.9719 - val_acc: 0.7370\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8960 - acc: 0.7855 - val_loss: 0.9736 - val_acc: 0.7420\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8982 - acc: 0.7820 - val_loss: 1.0020 - val_acc: 0.7260\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8990 - acc: 0.7835 - val_loss: 0.9878 - val_acc: 0.7470\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8978 - acc: 0.7844 - val_loss: 0.9729 - val_acc: 0.7470\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8958 - acc: 0.7836 - val_loss: 0.9685 - val_acc: 0.7440\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8963 - acc: 0.7837 - val_loss: 0.9700 - val_acc: 0.7410\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8957 - acc: 0.7852 - val_loss: 0.9714 - val_acc: 0.7530\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8957 - acc: 0.7844 - val_loss: 0.9681 - val_acc: 0.7360\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8952 - acc: 0.7867 - val_loss: 0.9841 - val_acc: 0.7310\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8949 - acc: 0.7861 - val_loss: 0.9668 - val_acc: 0.7430\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8951 - acc: 0.7825 - val_loss: 0.9957 - val_acc: 0.7340\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8955 - acc: 0.7859 - val_loss: 0.9727 - val_acc: 0.7430\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8958 - acc: 0.7864 - val_loss: 0.9686 - val_acc: 0.7480\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8937 - acc: 0.7840 - val_loss: 0.9672 - val_acc: 0.7440\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8948 - acc: 0.7859 - val_loss: 0.9731 - val_acc: 0.7440\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8937 - acc: 0.7865 - val_loss: 0.9809 - val_acc: 0.7390\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8939 - acc: 0.7839 - val_loss: 0.9675 - val_acc: 0.7390\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8935 - acc: 0.7860 - val_loss: 0.9694 - val_acc: 0.7460\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8929 - acc: 0.7865 - val_loss: 0.9686 - val_acc: 0.7440\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8924 - acc: 0.7860 - val_loss: 0.9677 - val_acc: 0.7390\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8932 - acc: 0.7856 - val_loss: 0.9702 - val_acc: 0.7500\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8940 - acc: 0.7844 - val_loss: 0.9649 - val_acc: 0.7480\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8921 - acc: 0.7867 - val_loss: 0.9764 - val_acc: 0.7410\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8919 - acc: 0.7864 - val_loss: 0.9697 - val_acc: 0.7380\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8926 - acc: 0.7845 - val_loss: 0.9734 - val_acc: 0.7450\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8924 - acc: 0.7857 - val_loss: 0.9811 - val_acc: 0.7290\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8929 - acc: 0.7855 - val_loss: 0.9776 - val_acc: 0.7400\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8893 - acc: 0.7876 - val_loss: 0.9745 - val_acc: 0.7400\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8913 - acc: 0.7869 - val_loss: 0.9715 - val_acc: 0.7380\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8912 - acc: 0.7861 - val_loss: 0.9762 - val_acc: 0.7460\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8904 - acc: 0.7899 - val_loss: 0.9742 - val_acc: 0.7410\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8899 - acc: 0.7868 - val_loss: 0.9659 - val_acc: 0.7490\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8904 - acc: 0.7880 - val_loss: 1.0040 - val_acc: 0.7300\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8922 - acc: 0.7879 - val_loss: 0.9718 - val_acc: 0.7420\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8896 - acc: 0.7891 - val_loss: 0.9759 - val_acc: 0.7420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8893 - acc: 0.7865 - val_loss: 0.9682 - val_acc: 0.7500\n",
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8881 - acc: 0.7861 - val_loss: 0.9686 - val_acc: 0.7490\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8879 - acc: 0.7869 - val_loss: 0.9692 - val_acc: 0.7460\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8896 - acc: 0.7873 - val_loss: 0.9822 - val_acc: 0.7340\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8881 - acc: 0.7873 - val_loss: 0.9651 - val_acc: 0.7470\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8872 - acc: 0.7875 - val_loss: 0.9616 - val_acc: 0.7400\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8880 - acc: 0.788 - 0s 35us/step - loss: 0.8878 - acc: 0.7888 - val_loss: 0.9787 - val_acc: 0.7490\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8883 - acc: 0.7857 - val_loss: 0.9948 - val_acc: 0.7410\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8886 - acc: 0.7855 - val_loss: 0.9626 - val_acc: 0.7430\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8859 - acc: 0.7888 - val_loss: 0.9613 - val_acc: 0.7430\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8882 - acc: 0.7883 - val_loss: 0.9827 - val_acc: 0.7340\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8867 - acc: 0.7855 - val_loss: 1.0008 - val_acc: 0.7290\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8867 - acc: 0.7893 - val_loss: 0.9749 - val_acc: 0.7380\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8870 - acc: 0.7872 - val_loss: 1.0244 - val_acc: 0.7160\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8878 - acc: 0.7877 - val_loss: 0.9607 - val_acc: 0.7430\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8856 - acc: 0.7885 - val_loss: 0.9638 - val_acc: 0.7430\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8850 - acc: 0.7895 - val_loss: 0.9681 - val_acc: 0.7480\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8854 - acc: 0.7896 - val_loss: 1.0035 - val_acc: 0.7320\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8859 - acc: 0.7884 - val_loss: 0.9623 - val_acc: 0.7480\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8845 - acc: 0.7892 - val_loss: 0.9709 - val_acc: 0.7450\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8838 - acc: 0.7892 - val_loss: 0.9676 - val_acc: 0.7440\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8841 - acc: 0.7899 - val_loss: 0.9596 - val_acc: 0.7430\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8852 - acc: 0.7880 - val_loss: 1.0071 - val_acc: 0.7220\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8859 - acc: 0.7867 - val_loss: 0.9587 - val_acc: 0.7440\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8842 - acc: 0.7904 - val_loss: 0.9732 - val_acc: 0.7480\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8843 - acc: 0.7876 - val_loss: 0.9603 - val_acc: 0.7470\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8837 - acc: 0.7889 - val_loss: 0.9580 - val_acc: 0.7390\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8825 - acc: 0.7909 - val_loss: 0.9694 - val_acc: 0.7410\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8853 - acc: 0.7892 - val_loss: 0.9698 - val_acc: 0.7360\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8841 - acc: 0.7899 - val_loss: 0.9809 - val_acc: 0.7470\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8815 - acc: 0.7896 - val_loss: 0.9575 - val_acc: 0.7480\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8837 - acc: 0.7908 - val_loss: 0.9575 - val_acc: 0.7500\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8818 - acc: 0.7901 - val_loss: 0.9678 - val_acc: 0.7380\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8814 - acc: 0.7904 - val_loss: 0.9868 - val_acc: 0.7310\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8820 - acc: 0.7899 - val_loss: 0.9580 - val_acc: 0.7500\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8806 - acc: 0.7917 - val_loss: 0.9797 - val_acc: 0.7320\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8807 - acc: 0.7917 - val_loss: 0.9592 - val_acc: 0.7450\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8804 - acc: 0.7920 - val_loss: 0.9559 - val_acc: 0.7450\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8798 - acc: 0.7920 - val_loss: 0.9840 - val_acc: 0.7320\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8830 - acc: 0.7903 - val_loss: 0.9641 - val_acc: 0.7460\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8823 - acc: 0.7899 - val_loss: 0.9697 - val_acc: 0.7510\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8795 - acc: 0.7929 - val_loss: 0.9636 - val_acc: 0.7430\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8801 - acc: 0.7877 - val_loss: 0.9566 - val_acc: 0.7420\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8792 - acc: 0.7907 - val_loss: 0.9577 - val_acc: 0.7410\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8796 - acc: 0.7916 - val_loss: 0.9640 - val_acc: 0.7480\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8796 - acc: 0.7904 - val_loss: 0.9623 - val_acc: 0.7460\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8791 - acc: 0.7923 - val_loss: 0.9586 - val_acc: 0.7480\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8756 - acc: 0.793 - 0s 39us/step - loss: 0.8786 - acc: 0.7919 - val_loss: 0.9779 - val_acc: 0.7420\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8804 - acc: 0.7895 - val_loss: 0.9680 - val_acc: 0.7530\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8796 - acc: 0.7920 - val_loss: 0.9585 - val_acc: 0.7440\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8777 - acc: 0.7933 - val_loss: 0.9619 - val_acc: 0.7520\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8773 - acc: 0.7917 - val_loss: 0.9676 - val_acc: 0.7440\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8798 - acc: 0.7899 - val_loss: 0.9655 - val_acc: 0.7420\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8781 - acc: 0.7909 - val_loss: 0.9655 - val_acc: 0.7480\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8780 - acc: 0.7896 - val_loss: 0.9598 - val_acc: 0.7510\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8771 - acc: 0.7959 - val_loss: 0.9644 - val_acc: 0.7460\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8786 - acc: 0.7917 - val_loss: 0.9726 - val_acc: 0.7330\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8782 - acc: 0.7908 - val_loss: 0.9614 - val_acc: 0.7450\n",
      "Epoch 589/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8759 - acc: 0.7931 - val_loss: 0.9602 - val_acc: 0.7470\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8766 - acc: 0.7931 - val_loss: 0.9570 - val_acc: 0.7520\n",
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8754 - acc: 0.7965 - val_loss: 0.9610 - val_acc: 0.7510\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8752 - acc: 0.7929 - val_loss: 0.9554 - val_acc: 0.7460\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8755 - acc: 0.7907 - val_loss: 0.9616 - val_acc: 0.7440\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8756 - acc: 0.7959 - val_loss: 0.9690 - val_acc: 0.7380\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8745 - acc: 0.7917 - val_loss: 0.9542 - val_acc: 0.7460\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8763 - acc: 0.7928 - val_loss: 0.9603 - val_acc: 0.7480\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8768 - acc: 0.7881 - val_loss: 0.9840 - val_acc: 0.7430\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8757 - acc: 0.7928 - val_loss: 0.9606 - val_acc: 0.7460\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8731 - acc: 0.7952 - val_loss: 0.9524 - val_acc: 0.7430\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8741 - acc: 0.7928 - val_loss: 0.9568 - val_acc: 0.7440\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8727 - acc: 0.7949 - val_loss: 0.9924 - val_acc: 0.7290\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8751 - acc: 0.7961 - val_loss: 0.9575 - val_acc: 0.7480\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8749 - acc: 0.7936 - val_loss: 0.9571 - val_acc: 0.7500\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8731 - acc: 0.7939 - val_loss: 0.9574 - val_acc: 0.7510\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8729 - acc: 0.7928 - val_loss: 0.9560 - val_acc: 0.7490\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8733 - acc: 0.7951 - val_loss: 0.9872 - val_acc: 0.7320\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8754 - acc: 0.7940 - val_loss: 0.9520 - val_acc: 0.7430\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8729 - acc: 0.7936 - val_loss: 0.9928 - val_acc: 0.7280\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8765 - acc: 0.7905 - val_loss: 0.9549 - val_acc: 0.7420\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8725 - acc: 0.7927 - val_loss: 0.9780 - val_acc: 0.7450\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8729 - acc: 0.7941 - val_loss: 0.9613 - val_acc: 0.7420\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8736 - acc: 0.7917 - val_loss: 0.9529 - val_acc: 0.7420\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8733 - acc: 0.7947 - val_loss: 0.9857 - val_acc: 0.7340\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8716 - acc: 0.7948 - val_loss: 0.9540 - val_acc: 0.7500\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8722 - acc: 0.7947 - val_loss: 0.9543 - val_acc: 0.7460\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8703 - acc: 0.7931 - val_loss: 0.9532 - val_acc: 0.7440\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8703 - acc: 0.7923 - val_loss: 0.9496 - val_acc: 0.7520\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8706 - acc: 0.7940 - val_loss: 0.9902 - val_acc: 0.7300\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8720 - acc: 0.7944 - val_loss: 0.9542 - val_acc: 0.7410\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8699 - acc: 0.7913 - val_loss: 0.9609 - val_acc: 0.7470\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8706 - acc: 0.7959 - val_loss: 0.9546 - val_acc: 0.7520\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8714 - acc: 0.7933 - val_loss: 0.9514 - val_acc: 0.7470\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8685 - acc: 0.7961 - val_loss: 0.9595 - val_acc: 0.7400\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8685 - acc: 0.7924 - val_loss: 0.9635 - val_acc: 0.7440\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8699 - acc: 0.7939 - val_loss: 0.9587 - val_acc: 0.7520\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8676 - acc: 0.7956 - val_loss: 0.9547 - val_acc: 0.7530\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8703 - acc: 0.7944 - val_loss: 0.9568 - val_acc: 0.7380\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8686 - acc: 0.7951 - val_loss: 0.9615 - val_acc: 0.7500\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8681 - acc: 0.7984 - val_loss: 0.9491 - val_acc: 0.7490\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8659 - acc: 0.7932 - val_loss: 0.9567 - val_acc: 0.7530\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8685 - acc: 0.7951 - val_loss: 0.9474 - val_acc: 0.7490\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8679 - acc: 0.7960 - val_loss: 0.9543 - val_acc: 0.7510\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8686 - acc: 0.7951 - val_loss: 0.9602 - val_acc: 0.7450\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8675 - acc: 0.7944 - val_loss: 0.9551 - val_acc: 0.7530\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8714 - acc: 0.7940 - val_loss: 0.9582 - val_acc: 0.7550\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8667 - acc: 0.7967 - val_loss: 0.9855 - val_acc: 0.7350\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8694 - acc: 0.7948 - val_loss: 0.9492 - val_acc: 0.7530\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8654 - acc: 0.7972 - val_loss: 0.9499 - val_acc: 0.7560\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8670 - acc: 0.7964 - val_loss: 0.9521 - val_acc: 0.7500\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8675 - acc: 0.7969 - val_loss: 0.9773 - val_acc: 0.7420\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8677 - acc: 0.7963 - val_loss: 0.9496 - val_acc: 0.7480\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8664 - acc: 0.7955 - val_loss: 0.9741 - val_acc: 0.7430\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8679 - acc: 0.7935 - val_loss: 0.9517 - val_acc: 0.7480\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8649 - acc: 0.7967 - val_loss: 0.9498 - val_acc: 0.7510\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8653 - acc: 0.7953 - val_loss: 0.9488 - val_acc: 0.7440\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8641 - acc: 0.7952 - val_loss: 0.9682 - val_acc: 0.7490\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8660 - acc: 0.7947 - val_loss: 0.9641 - val_acc: 0.7320\n",
      "Epoch 648/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8642 - acc: 0.7967 - val_loss: 0.9498 - val_acc: 0.7530\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8632 - acc: 0.7965 - val_loss: 0.9571 - val_acc: 0.7480\n",
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8638 - acc: 0.7963 - val_loss: 0.9579 - val_acc: 0.7480\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8649 - acc: 0.7963 - val_loss: 0.9585 - val_acc: 0.7410\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8657 - acc: 0.7996 - val_loss: 0.9464 - val_acc: 0.7540\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8632 - acc: 0.7957 - val_loss: 0.9544 - val_acc: 0.7600\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8656 - acc: 0.7971 - val_loss: 0.9590 - val_acc: 0.7490\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8649 - acc: 0.7965 - val_loss: 0.9577 - val_acc: 0.7500\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8624 - acc: 0.7948 - val_loss: 0.9511 - val_acc: 0.7520\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8632 - acc: 0.7973 - val_loss: 0.9795 - val_acc: 0.7470\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8654 - acc: 0.7977 - val_loss: 0.9495 - val_acc: 0.7460\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8653 - acc: 0.7953 - val_loss: 0.9545 - val_acc: 0.7460\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8622 - acc: 0.7967 - val_loss: 0.9503 - val_acc: 0.7510\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8642 - acc: 0.7976 - val_loss: 0.9471 - val_acc: 0.7530\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8622 - acc: 0.7975 - val_loss: 0.9469 - val_acc: 0.7490\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8615 - acc: 0.7955 - val_loss: 0.9502 - val_acc: 0.7570\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8603 - acc: 0.7975 - val_loss: 0.9473 - val_acc: 0.7540\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8622 - acc: 0.7955 - val_loss: 0.9578 - val_acc: 0.7470\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8614 - acc: 0.7969 - val_loss: 0.9678 - val_acc: 0.7410\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8655 - acc: 0.7931 - val_loss: 0.9731 - val_acc: 0.7320\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8642 - acc: 0.7964 - val_loss: 0.9509 - val_acc: 0.7520\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8610 - acc: 0.7984 - val_loss: 0.9611 - val_acc: 0.7470\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8615 - acc: 0.8004 - val_loss: 0.9700 - val_acc: 0.7380\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8622 - acc: 0.7984 - val_loss: 0.9489 - val_acc: 0.7520\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8603 - acc: 0.7965 - val_loss: 0.9527 - val_acc: 0.7580\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8637 - acc: 0.7953 - val_loss: 0.9500 - val_acc: 0.7570\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8607 - acc: 0.7977 - val_loss: 0.9509 - val_acc: 0.7470\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8598 - acc: 0.7991 - val_loss: 0.9639 - val_acc: 0.7480\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8603 - acc: 0.8007 - val_loss: 0.9437 - val_acc: 0.7540\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8581 - acc: 0.7992 - val_loss: 0.9559 - val_acc: 0.7500\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8586 - acc: 0.7987 - val_loss: 0.9461 - val_acc: 0.7550\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8589 - acc: 0.7989 - val_loss: 0.9602 - val_acc: 0.7430\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8595 - acc: 0.7989 - val_loss: 0.9458 - val_acc: 0.7520\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8588 - acc: 0.7995 - val_loss: 0.9454 - val_acc: 0.7500\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8564 - acc: 0.8005 - val_loss: 0.9480 - val_acc: 0.7500\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8569 - acc: 0.7987 - val_loss: 0.9446 - val_acc: 0.7540\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8572 - acc: 0.7985 - val_loss: 0.9497 - val_acc: 0.7590\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8593 - acc: 0.8015 - val_loss: 0.9494 - val_acc: 0.7450\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8596 - acc: 0.7984 - val_loss: 0.9504 - val_acc: 0.7480\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8574 - acc: 0.8000 - val_loss: 0.9467 - val_acc: 0.7520\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8582 - acc: 0.7968 - val_loss: 0.9466 - val_acc: 0.7500\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8553 - acc: 0.8007 - val_loss: 0.9581 - val_acc: 0.7480\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8574 - acc: 0.7973 - val_loss: 0.9625 - val_acc: 0.7430\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8557 - acc: 0.8016 - val_loss: 0.9520 - val_acc: 0.7490\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8563 - acc: 0.7989 - val_loss: 0.9599 - val_acc: 0.7570\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8546 - acc: 0.8000 - val_loss: 0.9674 - val_acc: 0.7430\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8574 - acc: 0.8024 - val_loss: 0.9470 - val_acc: 0.7460\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8567 - acc: 0.8000 - val_loss: 0.9563 - val_acc: 0.7580\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8559 - acc: 0.8007 - val_loss: 0.9444 - val_acc: 0.7450\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8545 - acc: 0.7989 - val_loss: 0.9451 - val_acc: 0.7570\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8567 - acc: 0.7995 - val_loss: 0.9598 - val_acc: 0.7480\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8580 - acc: 0.8008 - val_loss: 0.9730 - val_acc: 0.7380\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8573 - acc: 0.7985 - val_loss: 0.9571 - val_acc: 0.7430\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8564 - acc: 0.8000 - val_loss: 0.9423 - val_acc: 0.7520\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8542 - acc: 0.8005 - val_loss: 0.9541 - val_acc: 0.7580\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8549 - acc: 0.7965 - val_loss: 0.9841 - val_acc: 0.7360\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8583 - acc: 0.7971 - val_loss: 0.9637 - val_acc: 0.7400\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8546 - acc: 0.8007 - val_loss: 0.9608 - val_acc: 0.7580\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8535 - acc: 0.8023 - val_loss: 0.9559 - val_acc: 0.7470\n",
      "Epoch 707/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8539 - acc: 0.8033 - val_loss: 0.9491 - val_acc: 0.7540\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8527 - acc: 0.7991 - val_loss: 0.9592 - val_acc: 0.7530\n",
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8542 - acc: 0.7999 - val_loss: 0.9464 - val_acc: 0.7570\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8540 - acc: 0.7961 - val_loss: 0.9471 - val_acc: 0.7610\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8531 - acc: 0.8017 - val_loss: 0.9411 - val_acc: 0.7480\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8526 - acc: 0.8003 - val_loss: 0.9477 - val_acc: 0.7510\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8510 - acc: 0.8024 - val_loss: 0.9479 - val_acc: 0.7520\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8527 - acc: 0.8008 - val_loss: 0.9465 - val_acc: 0.7590\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8528 - acc: 0.7997 - val_loss: 0.9605 - val_acc: 0.7390\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8533 - acc: 0.8021 - val_loss: 0.9457 - val_acc: 0.7610\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8504 - acc: 0.8007 - val_loss: 0.9393 - val_acc: 0.7600\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8501 - acc: 0.8027 - val_loss: 0.9515 - val_acc: 0.7520\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8524 - acc: 0.8004 - val_loss: 0.9457 - val_acc: 0.7590\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8501 - acc: 0.8052 - val_loss: 0.9442 - val_acc: 0.7530\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8493 - acc: 0.8029 - val_loss: 0.9465 - val_acc: 0.7590\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8539 - acc: 0.7973 - val_loss: 0.9453 - val_acc: 0.7590\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8490 - acc: 0.8033 - val_loss: 0.9740 - val_acc: 0.7420\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8509 - acc: 0.8008 - val_loss: 0.9437 - val_acc: 0.7550\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8494 - acc: 0.8012 - val_loss: 0.9530 - val_acc: 0.7470\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8515 - acc: 0.8024 - val_loss: 0.9577 - val_acc: 0.7580\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8514 - acc: 0.8033 - val_loss: 0.9526 - val_acc: 0.7530\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8479 - acc: 0.8035 - val_loss: 0.9462 - val_acc: 0.7520\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8527 - acc: 0.8011 - val_loss: 0.9397 - val_acc: 0.7610\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8478 - acc: 0.8023 - val_loss: 0.9394 - val_acc: 0.7590\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8483 - acc: 0.8007 - val_loss: 0.9447 - val_acc: 0.7570\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8484 - acc: 0.8004 - val_loss: 0.9440 - val_acc: 0.7640\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8506 - acc: 0.8005 - val_loss: 0.9385 - val_acc: 0.7550\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8477 - acc: 0.8031 - val_loss: 0.9494 - val_acc: 0.7590\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8464 - acc: 0.8032 - val_loss: 0.9503 - val_acc: 0.7560\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8478 - acc: 0.8024 - val_loss: 0.9739 - val_acc: 0.7360\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8488 - acc: 0.8017 - val_loss: 0.9524 - val_acc: 0.7450\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8487 - acc: 0.8016 - val_loss: 0.9503 - val_acc: 0.7520\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8468 - acc: 0.8043 - val_loss: 0.9397 - val_acc: 0.7600\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8484 - acc: 0.7996 - val_loss: 0.9480 - val_acc: 0.7530\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8465 - acc: 0.8020 - val_loss: 0.9857 - val_acc: 0.7360\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8553 - acc: 0.7997 - val_loss: 0.9826 - val_acc: 0.7370\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8494 - acc: 0.8001 - val_loss: 1.0558 - val_acc: 0.7060\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8509 - acc: 0.8024 - val_loss: 0.9370 - val_acc: 0.7590\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8513 - acc: 0.8021 - val_loss: 0.9425 - val_acc: 0.7490\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8457 - acc: 0.8024 - val_loss: 0.9612 - val_acc: 0.7370\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8490 - acc: 0.8040 - val_loss: 0.9347 - val_acc: 0.7580\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8474 - acc: 0.8015 - val_loss: 0.9435 - val_acc: 0.7510\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8466 - acc: 0.8007 - val_loss: 0.9530 - val_acc: 0.7460\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8456 - acc: 0.8023 - val_loss: 0.9357 - val_acc: 0.7580\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8452 - acc: 0.8040 - val_loss: 0.9342 - val_acc: 0.7560\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8452 - acc: 0.8005 - val_loss: 0.9355 - val_acc: 0.7530\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8453 - acc: 0.8043 - val_loss: 0.9615 - val_acc: 0.7480\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8472 - acc: 0.8040 - val_loss: 0.9782 - val_acc: 0.7430\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8482 - acc: 0.8036 - val_loss: 0.9477 - val_acc: 0.7520\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8431 - acc: 0.8061 - val_loss: 0.9434 - val_acc: 0.7490\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8439 - acc: 0.8040 - val_loss: 0.9425 - val_acc: 0.7560\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8426 - acc: 0.8052 - val_loss: 0.9446 - val_acc: 0.7510\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8442 - acc: 0.8017 - val_loss: 0.9579 - val_acc: 0.7500\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8463 - acc: 0.8020 - val_loss: 0.9659 - val_acc: 0.7460\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8441 - acc: 0.8033 - val_loss: 0.9541 - val_acc: 0.7510\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8429 - acc: 0.8049 - val_loss: 0.9437 - val_acc: 0.7580\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8431 - acc: 0.8049 - val_loss: 0.9468 - val_acc: 0.7520\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8442 - acc: 0.8060 - val_loss: 0.9359 - val_acc: 0.7500\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8414 - acc: 0.8039 - val_loss: 0.9392 - val_acc: 0.7550\n",
      "Epoch 766/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8403 - acc: 0.8061 - val_loss: 0.9647 - val_acc: 0.7340\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8489 - acc: 0.8008 - val_loss: 0.9358 - val_acc: 0.7620\n",
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8409 - acc: 0.8037 - val_loss: 0.9439 - val_acc: 0.7520\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8406 - acc: 0.8053 - val_loss: 0.9601 - val_acc: 0.7490\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8411 - acc: 0.8068 - val_loss: 0.9571 - val_acc: 0.7500\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8436 - acc: 0.8041 - val_loss: 0.9515 - val_acc: 0.7530\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8444 - acc: 0.8063 - val_loss: 0.9712 - val_acc: 0.7460\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8430 - acc: 0.8053 - val_loss: 0.9440 - val_acc: 0.7580\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8437 - acc: 0.8037 - val_loss: 0.9565 - val_acc: 0.7490\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8419 - acc: 0.8055 - val_loss: 0.9338 - val_acc: 0.7610\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8423 - acc: 0.8049 - val_loss: 0.9493 - val_acc: 0.7480\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8467 - acc: 0.8041 - val_loss: 0.9337 - val_acc: 0.7610\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8400 - acc: 0.8043 - val_loss: 0.9461 - val_acc: 0.7560\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8404 - acc: 0.8047 - val_loss: 0.9371 - val_acc: 0.7490\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8438 - acc: 0.8012 - val_loss: 0.9441 - val_acc: 0.7550\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8419 - acc: 0.8041 - val_loss: 0.9565 - val_acc: 0.7430\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8397 - acc: 0.8059 - val_loss: 0.9397 - val_acc: 0.7540\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8387 - acc: 0.8057 - val_loss: 0.9516 - val_acc: 0.7560\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8396 - acc: 0.8048 - val_loss: 0.9527 - val_acc: 0.7480\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8421 - acc: 0.8021 - val_loss: 0.9421 - val_acc: 0.7470\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8413 - acc: 0.8064 - val_loss: 0.9423 - val_acc: 0.7560\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8412 - acc: 0.8055 - val_loss: 0.9344 - val_acc: 0.7550\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8384 - acc: 0.8077 - val_loss: 0.9398 - val_acc: 0.7530\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8385 - acc: 0.8076 - val_loss: 0.9502 - val_acc: 0.7470\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8388 - acc: 0.8059 - val_loss: 0.9524 - val_acc: 0.7450\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8389 - acc: 0.8049 - val_loss: 0.9458 - val_acc: 0.7500\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8383 - acc: 0.8057 - val_loss: 0.9347 - val_acc: 0.7550\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8375 - acc: 0.8071 - val_loss: 0.9522 - val_acc: 0.7450\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8395 - acc: 0.8055 - val_loss: 0.9345 - val_acc: 0.7550\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8371 - acc: 0.8071 - val_loss: 0.9352 - val_acc: 0.7550\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8378 - acc: 0.8080 - val_loss: 0.9356 - val_acc: 0.7570\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8404 - acc: 0.8064 - val_loss: 0.9446 - val_acc: 0.7460\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8390 - acc: 0.8081 - val_loss: 0.9641 - val_acc: 0.7480\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8367 - acc: 0.8096 - val_loss: 0.9595 - val_acc: 0.7440\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8437 - acc: 0.8023 - val_loss: 0.9514 - val_acc: 0.7500\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8412 - acc: 0.8060 - val_loss: 0.9563 - val_acc: 0.7460\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8410 - acc: 0.8055 - val_loss: 0.9415 - val_acc: 0.7620\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8357 - acc: 0.8091 - val_loss: 0.9499 - val_acc: 0.7550\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8354 - acc: 0.8083 - val_loss: 0.9329 - val_acc: 0.7520\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8351 - acc: 0.8105 - val_loss: 0.9382 - val_acc: 0.7540\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8351 - acc: 0.8073 - val_loss: 0.9340 - val_acc: 0.7540\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8347 - acc: 0.8059 - val_loss: 0.9350 - val_acc: 0.7590\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8365 - acc: 0.8059 - val_loss: 0.9290 - val_acc: 0.7580\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8347 - acc: 0.8105 - val_loss: 0.9331 - val_acc: 0.7570\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8376 - acc: 0.8059 - val_loss: 0.9443 - val_acc: 0.7430\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8351 - acc: 0.8068 - val_loss: 0.9506 - val_acc: 0.7530\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8344 - acc: 0.8088 - val_loss: 0.9458 - val_acc: 0.7430\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8343 - acc: 0.8076 - val_loss: 0.9327 - val_acc: 0.7530\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8358 - acc: 0.8056 - val_loss: 0.9321 - val_acc: 0.7560\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8338 - acc: 0.8065 - val_loss: 0.9303 - val_acc: 0.7540\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8351 - acc: 0.8085 - val_loss: 0.9333 - val_acc: 0.7530\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8341 - acc: 0.8083 - val_loss: 0.9809 - val_acc: 0.7320\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8415 - acc: 0.8031 - val_loss: 0.9482 - val_acc: 0.7430\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8355 - acc: 0.8093 - val_loss: 0.9401 - val_acc: 0.7550\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8328 - acc: 0.8111 - val_loss: 0.9301 - val_acc: 0.7550\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8316 - acc: 0.8100 - val_loss: 0.9314 - val_acc: 0.7620\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8342 - acc: 0.8077 - val_loss: 0.9358 - val_acc: 0.7540\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8349 - acc: 0.8059 - val_loss: 0.9349 - val_acc: 0.7570\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8325 - acc: 0.8092 - val_loss: 0.9450 - val_acc: 0.7530\n",
      "Epoch 825/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8334 - acc: 0.8052 - val_loss: 0.9364 - val_acc: 0.7520\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8337 - acc: 0.8088 - val_loss: 0.9828 - val_acc: 0.7430\n",
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8376 - acc: 0.8077 - val_loss: 0.9280 - val_acc: 0.7580\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8300 - acc: 0.8101 - val_loss: 0.9342 - val_acc: 0.7520\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8316 - acc: 0.8115 - val_loss: 0.9823 - val_acc: 0.7410\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8335 - acc: 0.8071 - val_loss: 0.9514 - val_acc: 0.7570\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8355 - acc: 0.8120 - val_loss: 0.9298 - val_acc: 0.7570\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8296 - acc: 0.8087 - val_loss: 0.9338 - val_acc: 0.7500\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8329 - acc: 0.8073 - val_loss: 0.9386 - val_acc: 0.7520\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8315 - acc: 0.8069 - val_loss: 0.9376 - val_acc: 0.7570\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8292 - acc: 0.8112 - val_loss: 0.9417 - val_acc: 0.7610\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8301 - acc: 0.8103 - val_loss: 0.9307 - val_acc: 0.7610\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8313 - acc: 0.8061 - val_loss: 0.9288 - val_acc: 0.7570\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8302 - acc: 0.8095 - val_loss: 0.9589 - val_acc: 0.7470\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8332 - acc: 0.8063 - val_loss: 0.9347 - val_acc: 0.7600\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8305 - acc: 0.8081 - val_loss: 0.9713 - val_acc: 0.7400\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8298 - acc: 0.8069 - val_loss: 0.9345 - val_acc: 0.7460\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8303 - acc: 0.8088 - val_loss: 0.9478 - val_acc: 0.7550\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8304 - acc: 0.8100 - val_loss: 0.9576 - val_acc: 0.7480\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8305 - acc: 0.8100 - val_loss: 0.9415 - val_acc: 0.7590\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8283 - acc: 0.8107 - val_loss: 0.9286 - val_acc: 0.7540\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8333 - acc: 0.8083 - val_loss: 0.9813 - val_acc: 0.7470\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8389 - acc: 0.8101 - val_loss: 0.9410 - val_acc: 0.7520\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8297 - acc: 0.8092 - val_loss: 0.9365 - val_acc: 0.7460\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8296 - acc: 0.8097 - val_loss: 0.9338 - val_acc: 0.7570\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8273 - acc: 0.8100 - val_loss: 0.9347 - val_acc: 0.7560\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8298 - acc: 0.8089 - val_loss: 0.9414 - val_acc: 0.7490\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8294 - acc: 0.8083 - val_loss: 0.9306 - val_acc: 0.7480\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8299 - acc: 0.8111 - val_loss: 0.9563 - val_acc: 0.7610\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8307 - acc: 0.8109 - val_loss: 0.9290 - val_acc: 0.7630\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8296 - acc: 0.8084 - val_loss: 0.9418 - val_acc: 0.7490\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8304 - acc: 0.8076 - val_loss: 0.9311 - val_acc: 0.7580\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8250 - acc: 0.8143 - val_loss: 0.9294 - val_acc: 0.7660\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8275 - acc: 0.8108 - val_loss: 0.9510 - val_acc: 0.7540\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8259 - acc: 0.8131 - val_loss: 0.9695 - val_acc: 0.7430\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8322 - acc: 0.8079 - val_loss: 0.9338 - val_acc: 0.7610\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8278 - acc: 0.8104 - val_loss: 0.9291 - val_acc: 0.7580\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8249 - acc: 0.8128 - val_loss: 0.9435 - val_acc: 0.7520\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8271 - acc: 0.8112 - val_loss: 0.9385 - val_acc: 0.7550\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8244 - acc: 0.8149 - val_loss: 0.9382 - val_acc: 0.7580\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8269 - acc: 0.8105 - val_loss: 0.9299 - val_acc: 0.7640\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8256 - acc: 0.8108 - val_loss: 0.9270 - val_acc: 0.7650\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8254 - acc: 0.8117 - val_loss: 0.9256 - val_acc: 0.7550\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8255 - acc: 0.8115 - val_loss: 0.9487 - val_acc: 0.7530\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8318 - acc: 0.8093 - val_loss: 0.9438 - val_acc: 0.7520\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8276 - acc: 0.8088 - val_loss: 0.9597 - val_acc: 0.7450\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8296 - acc: 0.8075 - val_loss: 1.0716 - val_acc: 0.7120\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8303 - acc: 0.8083 - val_loss: 0.9285 - val_acc: 0.7470\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8303 - acc: 0.8121 - val_loss: 0.9372 - val_acc: 0.7590\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8287 - acc: 0.8069 - val_loss: 0.9567 - val_acc: 0.7540\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8256 - acc: 0.8129 - val_loss: 0.9358 - val_acc: 0.7620\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8281 - acc: 0.8119 - val_loss: 1.0015 - val_acc: 0.7360\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8282 - acc: 0.8101 - val_loss: 0.9242 - val_acc: 0.7640\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8249 - acc: 0.8100 - val_loss: 0.9460 - val_acc: 0.7480\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8227 - acc: 0.8141 - val_loss: 0.9289 - val_acc: 0.7590\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8211 - acc: 0.8155 - val_loss: 0.9379 - val_acc: 0.7620\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8241 - acc: 0.8095 - val_loss: 0.9435 - val_acc: 0.7630\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8230 - acc: 0.8113 - val_loss: 0.9340 - val_acc: 0.7550\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8312 - acc: 0.8079 - val_loss: 0.9701 - val_acc: 0.7460\n",
      "Epoch 884/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8262 - acc: 0.8111 - val_loss: 0.9314 - val_acc: 0.7520\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8243 - acc: 0.8135 - val_loss: 0.9785 - val_acc: 0.7480\n",
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8269 - acc: 0.8120 - val_loss: 0.9575 - val_acc: 0.7490\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8239 - acc: 0.8152 - val_loss: 0.9500 - val_acc: 0.7530\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8250 - acc: 0.8123 - val_loss: 0.9332 - val_acc: 0.7580\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8237 - acc: 0.8139 - val_loss: 0.9441 - val_acc: 0.7490\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8229 - acc: 0.8143 - val_loss: 0.9414 - val_acc: 0.7600\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8216 - acc: 0.8144 - val_loss: 0.9400 - val_acc: 0.7560\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8243 - acc: 0.8145 - val_loss: 0.9949 - val_acc: 0.7340\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8264 - acc: 0.8109 - val_loss: 0.9282 - val_acc: 0.7590\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8214 - acc: 0.8160 - val_loss: 0.9342 - val_acc: 0.7570\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8308 - acc: 0.8100 - val_loss: 0.9688 - val_acc: 0.7480\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8285 - acc: 0.8112 - val_loss: 0.9412 - val_acc: 0.7530\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8267 - acc: 0.8107 - val_loss: 0.9429 - val_acc: 0.7540\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8204 - acc: 0.8144 - val_loss: 0.9357 - val_acc: 0.7570\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8255 - acc: 0.8101 - val_loss: 0.9400 - val_acc: 0.7550\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8212 - acc: 0.8155 - val_loss: 0.9432 - val_acc: 0.7510\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8230 - acc: 0.8135 - val_loss: 0.9235 - val_acc: 0.7620\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8264 - acc: 0.8089 - val_loss: 0.9259 - val_acc: 0.7630\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8223 - acc: 0.8144 - val_loss: 0.9376 - val_acc: 0.7590\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8267 - acc: 0.8123 - val_loss: 0.9303 - val_acc: 0.7550\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8269 - acc: 0.8111 - val_loss: 0.9312 - val_acc: 0.7540\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8216 - acc: 0.8115 - val_loss: 0.9301 - val_acc: 0.7590\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8205 - acc: 0.8156 - val_loss: 0.9556 - val_acc: 0.7490\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8263 - acc: 0.8097 - val_loss: 0.9284 - val_acc: 0.7630\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8211 - acc: 0.8132 - val_loss: 0.9765 - val_acc: 0.7480\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8231 - acc: 0.8161 - val_loss: 0.9476 - val_acc: 0.7580\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8254 - acc: 0.8099 - val_loss: 0.9270 - val_acc: 0.7600\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8218 - acc: 0.8109 - val_loss: 0.9294 - val_acc: 0.7620\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8196 - acc: 0.8140 - val_loss: 1.0023 - val_acc: 0.7260\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8232 - acc: 0.8144 - val_loss: 0.9582 - val_acc: 0.7410\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8163 - acc: 0.8152 - val_loss: 0.9575 - val_acc: 0.7490\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8203 - acc: 0.8151 - val_loss: 0.9425 - val_acc: 0.7590\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8177 - acc: 0.8135 - val_loss: 0.9651 - val_acc: 0.7480\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8218 - acc: 0.8139 - val_loss: 0.9261 - val_acc: 0.7590\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8189 - acc: 0.8125 - val_loss: 0.9402 - val_acc: 0.7530\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8270 - acc: 0.8108 - val_loss: 0.9287 - val_acc: 0.7630\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8159 - acc: 0.8133 - val_loss: 0.9555 - val_acc: 0.7490\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8262 - acc: 0.8107 - val_loss: 0.9287 - val_acc: 0.7600\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8204 - acc: 0.8163 - val_loss: 0.9269 - val_acc: 0.7590\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8168 - acc: 0.8165 - val_loss: 0.9306 - val_acc: 0.7620\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8167 - acc: 0.8128 - val_loss: 0.9624 - val_acc: 0.7500\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8217 - acc: 0.8111 - val_loss: 0.9504 - val_acc: 0.7470\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8240 - acc: 0.8124 - val_loss: 0.9445 - val_acc: 0.7560\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8162 - acc: 0.8149 - val_loss: 0.9388 - val_acc: 0.7610\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8155 - acc: 0.8167 - val_loss: 0.9268 - val_acc: 0.7570\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8201 - acc: 0.8141 - val_loss: 0.9320 - val_acc: 0.7590\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8195 - acc: 0.8135 - val_loss: 0.9386 - val_acc: 0.7590\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8231 - acc: 0.8137 - val_loss: 0.9273 - val_acc: 0.7520\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8205 - acc: 0.8137 - val_loss: 0.9701 - val_acc: 0.7420\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8230 - acc: 0.8105 - val_loss: 0.9497 - val_acc: 0.7470\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8241 - acc: 0.8113 - val_loss: 0.9646 - val_acc: 0.7450\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8188 - acc: 0.8121 - val_loss: 0.9643 - val_acc: 0.7490\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8161 - acc: 0.8164 - val_loss: 0.9283 - val_acc: 0.7620\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8168 - acc: 0.8151 - val_loss: 0.9264 - val_acc: 0.7610\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8146 - acc: 0.8148 - val_loss: 0.9397 - val_acc: 0.7590\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8198 - acc: 0.8149 - val_loss: 0.9334 - val_acc: 0.7660\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8170 - acc: 0.8119 - val_loss: 0.9380 - val_acc: 0.7580\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8203 - acc: 0.8139 - val_loss: 0.9638 - val_acc: 0.7500\n",
      "Epoch 943/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8211 - acc: 0.8157 - val_loss: 0.9629 - val_acc: 0.7400\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8212 - acc: 0.8133 - val_loss: 0.9664 - val_acc: 0.7570\n",
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8262 - acc: 0.8147 - val_loss: 0.9834 - val_acc: 0.7480\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8191 - acc: 0.8172 - val_loss: 0.9660 - val_acc: 0.7370\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8143 - acc: 0.8169 - val_loss: 0.9281 - val_acc: 0.7540\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8154 - acc: 0.8173 - val_loss: 0.9465 - val_acc: 0.7570\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8185 - acc: 0.8147 - val_loss: 0.9532 - val_acc: 0.7540\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8159 - acc: 0.8147 - val_loss: 0.9462 - val_acc: 0.7430\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8145 - acc: 0.8149 - val_loss: 0.9397 - val_acc: 0.7570\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8135 - acc: 0.8185 - val_loss: 0.9348 - val_acc: 0.7520\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8151 - acc: 0.8161 - val_loss: 0.9343 - val_acc: 0.7620\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8163 - acc: 0.8140 - val_loss: 0.9308 - val_acc: 0.7550\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8136 - acc: 0.8167 - val_loss: 0.9200 - val_acc: 0.7680\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8193 - acc: 0.8148 - val_loss: 0.9831 - val_acc: 0.7440\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8230 - acc: 0.8120 - val_loss: 0.9459 - val_acc: 0.7490\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8258 - acc: 0.8109 - val_loss: 0.9861 - val_acc: 0.7430\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8144 - acc: 0.8183 - val_loss: 0.9313 - val_acc: 0.7570\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8120 - acc: 0.8155 - val_loss: 0.9354 - val_acc: 0.7550\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8134 - acc: 0.8176 - val_loss: 0.9339 - val_acc: 0.7690\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8123 - acc: 0.8183 - val_loss: 0.9249 - val_acc: 0.7660\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8130 - acc: 0.8160 - val_loss: 0.9975 - val_acc: 0.7340\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8227 - acc: 0.8157 - val_loss: 0.9214 - val_acc: 0.7660\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8122 - acc: 0.8189 - val_loss: 0.9695 - val_acc: 0.7410\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8146 - acc: 0.8179 - val_loss: 0.9387 - val_acc: 0.7660\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8107 - acc: 0.8180 - val_loss: 0.9489 - val_acc: 0.7550\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8209 - acc: 0.8144 - val_loss: 0.9270 - val_acc: 0.7590\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8155 - acc: 0.8120 - val_loss: 0.9313 - val_acc: 0.7630\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8159 - acc: 0.8176 - val_loss: 1.0923 - val_acc: 0.7010\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8302 - acc: 0.8059 - val_loss: 0.9345 - val_acc: 0.7600\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8172 - acc: 0.8165 - val_loss: 0.9248 - val_acc: 0.7620\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8114 - acc: 0.8159 - val_loss: 0.9550 - val_acc: 0.7560\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8182 - acc: 0.8160 - val_loss: 0.9236 - val_acc: 0.7670\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8142 - acc: 0.8147 - val_loss: 0.9753 - val_acc: 0.7410\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8127 - acc: 0.8144 - val_loss: 0.9263 - val_acc: 0.7550\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8135 - acc: 0.8168 - val_loss: 0.9323 - val_acc: 0.7660\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8103 - acc: 0.8171 - val_loss: 0.9249 - val_acc: 0.7590\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8117 - acc: 0.8183 - val_loss: 0.9302 - val_acc: 0.7590\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8143 - acc: 0.8168 - val_loss: 0.9315 - val_acc: 0.7620\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8121 - acc: 0.8169 - val_loss: 0.9238 - val_acc: 0.7600\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8178 - acc: 0.8163 - val_loss: 0.9419 - val_acc: 0.7500\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8142 - acc: 0.8144 - val_loss: 0.9844 - val_acc: 0.7360\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8190 - acc: 0.8144 - val_loss: 0.9245 - val_acc: 0.7650\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8119 - acc: 0.8171 - val_loss: 0.9208 - val_acc: 0.7640\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8084 - acc: 0.8203 - val_loss: 0.9342 - val_acc: 0.7580\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8163 - acc: 0.8163 - val_loss: 0.9214 - val_acc: 0.7560\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8063 - acc: 0.8208 - val_loss: 0.9342 - val_acc: 0.7580\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8139 - acc: 0.8156 - val_loss: 0.9452 - val_acc: 0.7570\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8103 - acc: 0.8204 - val_loss: 0.9895 - val_acc: 0.7440\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8176 - acc: 0.8159 - val_loss: 0.9285 - val_acc: 0.7600\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8081 - acc: 0.8192 - val_loss: 0.9431 - val_acc: 0.7510\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8125 - acc: 0.8157 - val_loss: 0.9223 - val_acc: 0.7660\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8144 - acc: 0.8141 - val_loss: 0.9227 - val_acc: 0.7640\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8114 - acc: 0.8213 - val_loss: 0.9490 - val_acc: 0.7490\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8111 - acc: 0.8173 - val_loss: 0.9230 - val_acc: 0.7620\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8100 - acc: 0.8188 - val_loss: 0.9273 - val_acc: 0.7570\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8113 - acc: 0.8193 - val_loss: 0.9633 - val_acc: 0.7540\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8163 - acc: 0.8144 - val_loss: 0.9987 - val_acc: 0.7370\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8253 - acc: 0.8124 - val_loss: 0.9396 - val_acc: 0.7610\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50,kernel_regularizer=regularizers.l1(0.005),\n",
    "                activation='relu', input_shape=(2000,)))\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l1(0.005),\n",
    "                activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step\n",
      "1500/1500 [==============================] - 0s 37us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.814813737042745, 0.8133333333651225],\n",
       " [0.9724235757191976, 0.7420000003178915])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)\n",
    "results_train, results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best result you've achieved so far, but you were training for quite a while! Next, experiment with dropout regularization to see if it offers any advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.9810 - acc: 0.1403 - val_loss: 1.9446 - val_acc: 0.1670\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.9548 - acc: 0.1625 - val_loss: 1.9329 - val_acc: 0.1820\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.9475 - acc: 0.1661 - val_loss: 1.9236 - val_acc: 0.2110\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.9420 - acc: 0.1729 - val_loss: 1.9167 - val_acc: 0.2260\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9297 - acc: 0.1891 - val_loss: 1.9079 - val_acc: 0.2330\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9215 - acc: 0.1987 - val_loss: 1.8986 - val_acc: 0.2340\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9159 - acc: 0.2024 - val_loss: 1.8911 - val_acc: 0.2420\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.9094 - acc: 0.2087 - val_loss: 1.8817 - val_acc: 0.2560\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9022 - acc: 0.2215 - val_loss: 1.8707 - val_acc: 0.2490\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.8954 - acc: 0.2260 - val_loss: 1.8579 - val_acc: 0.2570\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.8829 - acc: 0.2381 - val_loss: 1.8452 - val_acc: 0.2800\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.8718 - acc: 0.2500 - val_loss: 1.8297 - val_acc: 0.2950\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.8616 - acc: 0.2493 - val_loss: 1.8140 - val_acc: 0.3000\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8473 - acc: 0.2613 - val_loss: 1.7963 - val_acc: 0.3160\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.8335 - acc: 0.2675 - val_loss: 1.7768 - val_acc: 0.3360\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8207 - acc: 0.2764 - val_loss: 1.7562 - val_acc: 0.3500\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8056 - acc: 0.2791 - val_loss: 1.7354 - val_acc: 0.3690\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.7947 - acc: 0.2799 - val_loss: 1.7141 - val_acc: 0.3920\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.7776 - acc: 0.2984 - val_loss: 1.6892 - val_acc: 0.4140\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.7486 - acc: 0.3164 - val_loss: 1.6636 - val_acc: 0.4470\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.7365 - acc: 0.3168 - val_loss: 1.6379 - val_acc: 0.4520\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.7228 - acc: 0.3253 - val_loss: 1.6137 - val_acc: 0.4780\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.6956 - acc: 0.3360 - val_loss: 1.5867 - val_acc: 0.4970\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6812 - acc: 0.3393 - val_loss: 1.5596 - val_acc: 0.5070\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6666 - acc: 0.3507 - val_loss: 1.5342 - val_acc: 0.5160\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6453 - acc: 0.3681 - val_loss: 1.5063 - val_acc: 0.5410\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.6265 - acc: 0.3731 - val_loss: 1.4822 - val_acc: 0.5490\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.6105 - acc: 0.3763 - val_loss: 1.4569 - val_acc: 0.5490\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.5913 - acc: 0.3867 - val_loss: 1.4339 - val_acc: 0.5620\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5683 - acc: 0.3981 - val_loss: 1.4098 - val_acc: 0.5720\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5523 - acc: 0.4007 - val_loss: 1.3847 - val_acc: 0.5770\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5363 - acc: 0.4065 - val_loss: 1.3651 - val_acc: 0.5960\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5204 - acc: 0.4089 - val_loss: 1.3444 - val_acc: 0.5900\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4992 - acc: 0.4240 - val_loss: 1.3219 - val_acc: 0.5940\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4905 - acc: 0.4139 - val_loss: 1.3040 - val_acc: 0.6020\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4677 - acc: 0.4392 - val_loss: 1.2836 - val_acc: 0.6100\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4613 - acc: 0.4411 - val_loss: 1.2645 - val_acc: 0.6090\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4547 - acc: 0.4276 - val_loss: 1.2502 - val_acc: 0.6170\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4390 - acc: 0.4453 - val_loss: 1.2334 - val_acc: 0.6210\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4204 - acc: 0.4512 - val_loss: 1.2148 - val_acc: 0.6190\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4112 - acc: 0.4576 - val_loss: 1.1966 - val_acc: 0.6240\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3880 - acc: 0.4649 - val_loss: 1.1782 - val_acc: 0.6320\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3724 - acc: 0.4829 - val_loss: 1.1641 - val_acc: 0.6360\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3695 - acc: 0.4767 - val_loss: 1.1471 - val_acc: 0.6430\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.3593 - acc: 0.4823 - val_loss: 1.1325 - val_acc: 0.6460\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.3481 - acc: 0.4824 - val_loss: 1.1207 - val_acc: 0.6530\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3302 - acc: 0.4955 - val_loss: 1.1062 - val_acc: 0.6530\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3229 - acc: 0.4968 - val_loss: 1.0939 - val_acc: 0.6610\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2984 - acc: 0.5052 - val_loss: 1.0792 - val_acc: 0.6610\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2834 - acc: 0.5053 - val_loss: 1.0656 - val_acc: 0.6650\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2797 - acc: 0.5071 - val_loss: 1.0536 - val_acc: 0.6750\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2792 - acc: 0.5108 - val_loss: 1.0448 - val_acc: 0.6740\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2667 - acc: 0.5161 - val_loss: 1.0339 - val_acc: 0.6760\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2532 - acc: 0.5208 - val_loss: 1.0229 - val_acc: 0.6810\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.2461 - acc: 0.5361 - val_loss: 1.0137 - val_acc: 0.6800\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.2365 - acc: 0.5304 - val_loss: 1.0040 - val_acc: 0.6880\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.2282 - acc: 0.5369 - val_loss: 0.9928 - val_acc: 0.6890\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.2172 - acc: 0.5425 - val_loss: 0.9811 - val_acc: 0.6890\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.2074 - acc: 0.5477 - val_loss: 0.9723 - val_acc: 0.7020\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.2002 - acc: 0.5487 - val_loss: 0.9641 - val_acc: 0.6950\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1941 - acc: 0.5473 - val_loss: 0.9562 - val_acc: 0.6960\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1902 - acc: 0.5525 - val_loss: 0.9469 - val_acc: 0.7030\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1796 - acc: 0.5569 - val_loss: 0.9384 - val_acc: 0.7050\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1773 - acc: 0.5504 - val_loss: 0.9299 - val_acc: 0.7110\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1558 - acc: 0.5639 - val_loss: 0.9201 - val_acc: 0.7100\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1474 - acc: 0.5635 - val_loss: 0.9134 - val_acc: 0.7220\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.1446 - acc: 0.5665 - val_loss: 0.9073 - val_acc: 0.7130\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.1394 - acc: 0.5689 - val_loss: 0.8986 - val_acc: 0.7160\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1382 - acc: 0.5688 - val_loss: 0.8917 - val_acc: 0.7180\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1456 - acc: 0.5728 - val_loss: 0.8894 - val_acc: 0.7170\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1071 - acc: 0.5868 - val_loss: 0.8811 - val_acc: 0.7230\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1118 - acc: 0.5900 - val_loss: 0.8744 - val_acc: 0.7260\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1166 - acc: 0.5857 - val_loss: 0.8671 - val_acc: 0.7280\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1025 - acc: 0.5867 - val_loss: 0.8616 - val_acc: 0.7300\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0854 - acc: 0.5955 - val_loss: 0.8532 - val_acc: 0.7370\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0952 - acc: 0.5899 - val_loss: 0.8503 - val_acc: 0.7260\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0770 - acc: 0.5980 - val_loss: 0.8415 - val_acc: 0.7350\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0684 - acc: 0.5971 - val_loss: 0.8386 - val_acc: 0.7290\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0842 - acc: 0.5905 - val_loss: 0.8337 - val_acc: 0.7360\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0584 - acc: 0.6085 - val_loss: 0.8261 - val_acc: 0.7450\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0461 - acc: 0.6107 - val_loss: 0.8212 - val_acc: 0.7470\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.0509 - acc: 0.6076 - val_loss: 0.8181 - val_acc: 0.7490\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0462 - acc: 0.6085 - val_loss: 0.8136 - val_acc: 0.7460\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0359 - acc: 0.6149 - val_loss: 0.8065 - val_acc: 0.7470\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0460 - acc: 0.6163 - val_loss: 0.8025 - val_acc: 0.7550\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0308 - acc: 0.6139 - val_loss: 0.8004 - val_acc: 0.7540\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0288 - acc: 0.6199 - val_loss: 0.7944 - val_acc: 0.7530\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0428 - acc: 0.6127 - val_loss: 0.7935 - val_acc: 0.7520\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0225 - acc: 0.6269 - val_loss: 0.7899 - val_acc: 0.7550\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.0144 - acc: 0.6239 - val_loss: 0.7827 - val_acc: 0.7540\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0054 - acc: 0.6219 - val_loss: 0.7795 - val_acc: 0.7540\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9885 - acc: 0.6332 - val_loss: 0.7738 - val_acc: 0.7580\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0024 - acc: 0.6235 - val_loss: 0.7700 - val_acc: 0.7600\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0117 - acc: 0.6267 - val_loss: 0.7701 - val_acc: 0.7600\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9916 - acc: 0.6301 - val_loss: 0.7664 - val_acc: 0.7590\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0069 - acc: 0.6255 - val_loss: 0.7635 - val_acc: 0.7610\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9822 - acc: 0.6387 - val_loss: 0.7587 - val_acc: 0.7640\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9790 - acc: 0.6415 - val_loss: 0.7532 - val_acc: 0.7610\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9746 - acc: 0.6357 - val_loss: 0.7532 - val_acc: 0.7560\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9639 - acc: 0.6433 - val_loss: 0.7473 - val_acc: 0.7600\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9755 - acc: 0.6320 - val_loss: 0.7430 - val_acc: 0.7650\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9569 - acc: 0.6455 - val_loss: 0.7399 - val_acc: 0.7640\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9728 - acc: 0.6329 - val_loss: 0.7388 - val_acc: 0.7610\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9436 - acc: 0.6536 - val_loss: 0.7365 - val_acc: 0.7660\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9562 - acc: 0.6453 - val_loss: 0.7334 - val_acc: 0.7610\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9507 - acc: 0.6511 - val_loss: 0.7299 - val_acc: 0.7640\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9559 - acc: 0.6475 - val_loss: 0.7292 - val_acc: 0.7670\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9437 - acc: 0.6477 - val_loss: 0.7256 - val_acc: 0.7670\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9377 - acc: 0.6537 - val_loss: 0.7228 - val_acc: 0.7700\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9451 - acc: 0.6437 - val_loss: 0.7187 - val_acc: 0.7700\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9544 - acc: 0.6468 - val_loss: 0.7187 - val_acc: 0.7750\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9393 - acc: 0.6499 - val_loss: 0.7195 - val_acc: 0.7700\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9309 - acc: 0.6605 - val_loss: 0.7135 - val_acc: 0.7730\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9127 - acc: 0.6676 - val_loss: 0.7080 - val_acc: 0.7730\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9236 - acc: 0.6600 - val_loss: 0.7066 - val_acc: 0.7710\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9109 - acc: 0.6596 - val_loss: 0.7052 - val_acc: 0.7740\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8955 - acc: 0.6679 - val_loss: 0.7016 - val_acc: 0.7760\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.9239 - acc: 0.6639 - val_loss: 0.7026 - val_acc: 0.7760\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9117 - acc: 0.6617 - val_loss: 0.7014 - val_acc: 0.7730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9005 - acc: 0.6688 - val_loss: 0.6991 - val_acc: 0.7730\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9007 - acc: 0.6624 - val_loss: 0.6979 - val_acc: 0.7730\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8952 - acc: 0.6691 - val_loss: 0.6965 - val_acc: 0.7710\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8917 - acc: 0.6723 - val_loss: 0.6915 - val_acc: 0.7750\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9061 - acc: 0.6687 - val_loss: 0.6930 - val_acc: 0.7780\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9051 - acc: 0.6669 - val_loss: 0.6897 - val_acc: 0.7830\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8771 - acc: 0.6759 - val_loss: 0.6864 - val_acc: 0.7780\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8696 - acc: 0.6863 - val_loss: 0.6851 - val_acc: 0.7770\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8699 - acc: 0.6833 - val_loss: 0.6815 - val_acc: 0.7790\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8726 - acc: 0.6776 - val_loss: 0.6794 - val_acc: 0.7820\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8719 - acc: 0.6836 - val_loss: 0.6779 - val_acc: 0.7820\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8800 - acc: 0.6761 - val_loss: 0.6761 - val_acc: 0.7770\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8555 - acc: 0.6831 - val_loss: 0.6747 - val_acc: 0.7810\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8527 - acc: 0.6905 - val_loss: 0.6749 - val_acc: 0.7790\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8571 - acc: 0.6816 - val_loss: 0.6736 - val_acc: 0.7830\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8570 - acc: 0.6795 - val_loss: 0.6721 - val_acc: 0.7750\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8599 - acc: 0.6787 - val_loss: 0.6722 - val_acc: 0.7810\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8639 - acc: 0.6816 - val_loss: 0.6716 - val_acc: 0.7750\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8544 - acc: 0.6819 - val_loss: 0.6694 - val_acc: 0.7770\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8389 - acc: 0.6868 - val_loss: 0.6668 - val_acc: 0.7790\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8383 - acc: 0.6876 - val_loss: 0.6646 - val_acc: 0.7810\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8374 - acc: 0.6933 - val_loss: 0.6632 - val_acc: 0.7780\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8470 - acc: 0.6900 - val_loss: 0.6621 - val_acc: 0.7780\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8476 - acc: 0.6945 - val_loss: 0.6620 - val_acc: 0.7800\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8461 - acc: 0.6853 - val_loss: 0.6609 - val_acc: 0.7790\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8450 - acc: 0.6917 - val_loss: 0.6602 - val_acc: 0.7760\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8311 - acc: 0.6977 - val_loss: 0.6568 - val_acc: 0.7780\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8177 - acc: 0.6979 - val_loss: 0.6564 - val_acc: 0.7810\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8389 - acc: 0.6972 - val_loss: 0.6540 - val_acc: 0.7780\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8352 - acc: 0.6933 - val_loss: 0.6533 - val_acc: 0.7810\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8183 - acc: 0.7021 - val_loss: 0.6535 - val_acc: 0.7840\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8131 - acc: 0.7009 - val_loss: 0.6511 - val_acc: 0.7810\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8204 - acc: 0.6892 - val_loss: 0.6541 - val_acc: 0.7810\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8171 - acc: 0.7044 - val_loss: 0.6484 - val_acc: 0.7790\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8055 - acc: 0.7048 - val_loss: 0.6459 - val_acc: 0.7770\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8239 - acc: 0.6976 - val_loss: 0.6472 - val_acc: 0.7810\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8222 - acc: 0.6975 - val_loss: 0.6476 - val_acc: 0.7800\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8248 - acc: 0.6977 - val_loss: 0.6488 - val_acc: 0.7810\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8105 - acc: 0.6979 - val_loss: 0.6471 - val_acc: 0.7800\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8125 - acc: 0.7021 - val_loss: 0.6455 - val_acc: 0.7810\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8021 - acc: 0.7052 - val_loss: 0.6452 - val_acc: 0.7830\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8051 - acc: 0.7007 - val_loss: 0.6424 - val_acc: 0.7820\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7912 - acc: 0.7105 - val_loss: 0.6427 - val_acc: 0.7850\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8093 - acc: 0.7033 - val_loss: 0.6442 - val_acc: 0.7840\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7939 - acc: 0.7111 - val_loss: 0.6419 - val_acc: 0.7850\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7939 - acc: 0.7024 - val_loss: 0.6384 - val_acc: 0.7860\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7789 - acc: 0.7141 - val_loss: 0.6380 - val_acc: 0.7840\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7946 - acc: 0.7080 - val_loss: 0.6388 - val_acc: 0.7840\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7788 - acc: 0.7112 - val_loss: 0.6344 - val_acc: 0.7850\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7982 - acc: 0.7051 - val_loss: 0.6362 - val_acc: 0.7830\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7943 - acc: 0.7080 - val_loss: 0.6377 - val_acc: 0.7840\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7991 - acc: 0.7045 - val_loss: 0.6352 - val_acc: 0.7800\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7852 - acc: 0.7119 - val_loss: 0.6350 - val_acc: 0.7820\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7635 - acc: 0.7147 - val_loss: 0.6356 - val_acc: 0.7790\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7846 - acc: 0.7080 - val_loss: 0.6324 - val_acc: 0.7810\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7948 - acc: 0.7124 - val_loss: 0.6323 - val_acc: 0.7810\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7650 - acc: 0.7195 - val_loss: 0.6336 - val_acc: 0.7790\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7782 - acc: 0.7133 - val_loss: 0.6291 - val_acc: 0.7850\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7664 - acc: 0.7192 - val_loss: 0.6294 - val_acc: 0.7810\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7666 - acc: 0.7184 - val_loss: 0.6272 - val_acc: 0.7870\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7704 - acc: 0.7189 - val_loss: 0.6285 - val_acc: 0.7820\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.7756 - acc: 0.7103 - val_loss: 0.6274 - val_acc: 0.7860\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.7659 - acc: 0.7131 - val_loss: 0.6263 - val_acc: 0.7800\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.7722 - acc: 0.7155 - val_loss: 0.6272 - val_acc: 0.7780\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.7506 - acc: 0.7233 - val_loss: 0.6258 - val_acc: 0.7840\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7576 - acc: 0.7253 - val_loss: 0.6255 - val_acc: 0.7800\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.7502 - acc: 0.7207 - val_loss: 0.6249 - val_acc: 0.7790\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7459 - acc: 0.7263 - val_loss: 0.6230 - val_acc: 0.7840\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.7550 - acc: 0.7225 - val_loss: 0.6238 - val_acc: 0.7790\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7670 - acc: 0.7191 - val_loss: 0.6234 - val_acc: 0.7810\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.7339 - acc: 0.7275 - val_loss: 0.6230 - val_acc: 0.7800\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.7437 - acc: 0.7251 - val_loss: 0.6216 - val_acc: 0.7820\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7481 - acc: 0.7251 - val_loss: 0.6223 - val_acc: 0.7830\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7525 - acc: 0.7219 - val_loss: 0.6226 - val_acc: 0.7830\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7420 - acc: 0.7344 - val_loss: 0.6206 - val_acc: 0.7840\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.7370 - acc: 0.7300 - val_loss: 0.6181 - val_acc: 0.7830\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.7481 - acc: 0.7205 - val_loss: 0.6177 - val_acc: 0.7830\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7339 - acc: 0.7292 - val_loss: 0.6169 - val_acc: 0.7840\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7414 - acc: 0.7265 - val_loss: 0.6201 - val_acc: 0.7830\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7318 - acc: 0.7300 - val_loss: 0.6181 - val_acc: 0.7820\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7394 - acc: 0.7289 - val_loss: 0.6162 - val_acc: 0.7830\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 38us/step\n",
      "1500/1500 [==============================] - 0s 37us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.4506875052928925, 0.8568000000317891], [0.6314767530759176, 0.768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)\n",
    "results_train, results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. You actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple your data set, and see what happens. Note that you are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 1.9105 - acc: 0.1905 - val_loss: 1.8737 - val_acc: 0.2570\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 1.8299 - acc: 0.2820 - val_loss: 1.7731 - val_acc: 0.3483\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 1.6932 - acc: 0.4172 - val_loss: 1.6033 - val_acc: 0.5010\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 1.4855 - acc: 0.5516 - val_loss: 1.3728 - val_acc: 0.6007\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 1.2581 - acc: 0.6308 - val_loss: 1.1668 - val_acc: 0.6557\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 1.0769 - acc: 0.6748 - val_loss: 1.0169 - val_acc: 0.6860\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.9480 - acc: 0.7035 - val_loss: 0.9134 - val_acc: 0.7023\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.8584 - acc: 0.7219 - val_loss: 0.8427 - val_acc: 0.7170\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.7947 - acc: 0.7358 - val_loss: 0.7901 - val_acc: 0.7280\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.7478 - acc: 0.7455 - val_loss: 0.7552 - val_acc: 0.7397\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.7117 - acc: 0.7530 - val_loss: 0.7262 - val_acc: 0.7413\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.6830 - acc: 0.7633 - val_loss: 0.7038 - val_acc: 0.7503\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.6597 - acc: 0.7698 - val_loss: 0.6868 - val_acc: 0.7517\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.6404 - acc: 0.7744 - val_loss: 0.6729 - val_acc: 0.7573\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.6238 - acc: 0.7796 - val_loss: 0.6605 - val_acc: 0.7610\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.6095 - acc: 0.7840 - val_loss: 0.6496 - val_acc: 0.7607\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5964 - acc: 0.7895 - val_loss: 0.6404 - val_acc: 0.7640\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5850 - acc: 0.7915 - val_loss: 0.6314 - val_acc: 0.7700\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5746 - acc: 0.7956 - val_loss: 0.6231 - val_acc: 0.7710\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5650 - acc: 0.7983 - val_loss: 0.6187 - val_acc: 0.7697\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5559 - acc: 0.8021 - val_loss: 0.6117 - val_acc: 0.7743\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5478 - acc: 0.8046 - val_loss: 0.6065 - val_acc: 0.7770\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5403 - acc: 0.8075 - val_loss: 0.6008 - val_acc: 0.7790\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5328 - acc: 0.8107 - val_loss: 0.5992 - val_acc: 0.7800\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5260 - acc: 0.8135 - val_loss: 0.5944 - val_acc: 0.7807\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5196 - acc: 0.8152 - val_loss: 0.5889 - val_acc: 0.7810\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.5132 - acc: 0.8181 - val_loss: 0.5870 - val_acc: 0.7870\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.5075 - acc: 0.8196 - val_loss: 0.5811 - val_acc: 0.7847\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.5019 - acc: 0.8221 - val_loss: 0.5794 - val_acc: 0.7857\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4964 - acc: 0.8239 - val_loss: 0.5773 - val_acc: 0.7877\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4914 - acc: 0.8254 - val_loss: 0.5738 - val_acc: 0.7900\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4866 - acc: 0.8275 - val_loss: 0.5711 - val_acc: 0.7930\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4819 - acc: 0.8291 - val_loss: 0.5684 - val_acc: 0.7933\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4774 - acc: 0.8305 - val_loss: 0.5669 - val_acc: 0.7927\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4730 - acc: 0.8311 - val_loss: 0.5639 - val_acc: 0.7940\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4687 - acc: 0.8335 - val_loss: 0.5632 - val_acc: 0.7957\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4643 - acc: 0.8343 - val_loss: 0.5616 - val_acc: 0.7993\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4606 - acc: 0.8362 - val_loss: 0.5587 - val_acc: 0.7953\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4566 - acc: 0.8377 - val_loss: 0.5575 - val_acc: 0.7960\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4531 - acc: 0.8385 - val_loss: 0.5555 - val_acc: 0.7977\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4493 - acc: 0.8410 - val_loss: 0.5550 - val_acc: 0.7973\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4459 - acc: 0.8427 - val_loss: 0.5532 - val_acc: 0.7993\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4425 - acc: 0.8417 - val_loss: 0.5533 - val_acc: 0.8007\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4394 - acc: 0.8435 - val_loss: 0.5520 - val_acc: 0.8013\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4363 - acc: 0.8441 - val_loss: 0.5515 - val_acc: 0.8020\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4331 - acc: 0.8457 - val_loss: 0.5485 - val_acc: 0.8010\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.4300 - acc: 0.8462 - val_loss: 0.5502 - val_acc: 0.8030\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4271 - acc: 0.8482 - val_loss: 0.5484 - val_acc: 0.8023\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4240 - acc: 0.8493 - val_loss: 0.5470 - val_acc: 0.8023\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.4213 - acc: 0.8495 - val_loss: 0.5465 - val_acc: 0.8043\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4183 - acc: 0.8507 - val_loss: 0.5478 - val_acc: 0.8027\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4159 - acc: 0.8515 - val_loss: 0.5499 - val_acc: 0.8040\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4130 - acc: 0.8529 - val_loss: 0.5460 - val_acc: 0.8067\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4105 - acc: 0.8542 - val_loss: 0.5458 - val_acc: 0.8097\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4082 - acc: 0.8540 - val_loss: 0.5439 - val_acc: 0.8073\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4053 - acc: 0.8563 - val_loss: 0.5459 - val_acc: 0.8083\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4030 - acc: 0.8565 - val_loss: 0.5436 - val_acc: 0.8093\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4008 - acc: 0.8569 - val_loss: 0.5441 - val_acc: 0.8087\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3985 - acc: 0.8586 - val_loss: 0.5437 - val_acc: 0.8067\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3958 - acc: 0.8597 - val_loss: 0.5444 - val_acc: 0.8057\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3939 - acc: 0.8600 - val_loss: 0.5441 - val_acc: 0.8083\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3915 - acc: 0.8610 - val_loss: 0.5429 - val_acc: 0.8063\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3892 - acc: 0.8613 - val_loss: 0.5432 - val_acc: 0.8073\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3870 - acc: 0.8625 - val_loss: 0.5456 - val_acc: 0.8080\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3850 - acc: 0.8645 - val_loss: 0.5449 - val_acc: 0.8060\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3835 - acc: 0.8639 - val_loss: 0.5446 - val_acc: 0.8057\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3811 - acc: 0.8648 - val_loss: 0.5435 - val_acc: 0.8073\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3793 - acc: 0.8661 - val_loss: 0.5447 - val_acc: 0.8093\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3776 - acc: 0.8662 - val_loss: 0.5462 - val_acc: 0.8070\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3751 - acc: 0.8675 - val_loss: 0.5473 - val_acc: 0.8057\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3732 - acc: 0.8676 - val_loss: 0.5462 - val_acc: 0.8060\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3717 - acc: 0.8678 - val_loss: 0.5449 - val_acc: 0.8080\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3699 - acc: 0.8682 - val_loss: 0.5462 - val_acc: 0.8080\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3680 - acc: 0.8697 - val_loss: 0.5494 - val_acc: 0.8057\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3660 - acc: 0.8703 - val_loss: 0.5476 - val_acc: 0.8087\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3641 - acc: 0.8715 - val_loss: 0.5483 - val_acc: 0.8083\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3626 - acc: 0.8724 - val_loss: 0.5467 - val_acc: 0.8093\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3610 - acc: 0.8726 - val_loss: 0.5499 - val_acc: 0.8057\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3595 - acc: 0.8722 - val_loss: 0.5484 - val_acc: 0.8070\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3576 - acc: 0.8735 - val_loss: 0.5519 - val_acc: 0.8073\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3559 - acc: 0.8750 - val_loss: 0.5530 - val_acc: 0.8057\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3545 - acc: 0.8741 - val_loss: 0.5507 - val_acc: 0.8067\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3527 - acc: 0.8753 - val_loss: 0.5503 - val_acc: 0.8077\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3512 - acc: 0.8756 - val_loss: 0.5514 - val_acc: 0.8053\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3497 - acc: 0.8769 - val_loss: 0.5518 - val_acc: 0.8077\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3481 - acc: 0.8780 - val_loss: 0.5524 - val_acc: 0.8063\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3465 - acc: 0.8782 - val_loss: 0.5525 - val_acc: 0.8063\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3451 - acc: 0.8777 - val_loss: 0.5541 - val_acc: 0.8063\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3435 - acc: 0.8788 - val_loss: 0.5554 - val_acc: 0.8053\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3425 - acc: 0.8792 - val_loss: 0.5553 - val_acc: 0.8060\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3409 - acc: 0.8793 - val_loss: 0.5567 - val_acc: 0.8057\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3393 - acc: 0.8801 - val_loss: 0.5569 - val_acc: 0.8043\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3379 - acc: 0.8807 - val_loss: 0.5573 - val_acc: 0.8060\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3363 - acc: 0.8822 - val_loss: 0.5595 - val_acc: 0.8020\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3352 - acc: 0.8822 - val_loss: 0.5593 - val_acc: 0.8027\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3338 - acc: 0.8831 - val_loss: 0.5661 - val_acc: 0.8007\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3323 - acc: 0.8834 - val_loss: 0.5623 - val_acc: 0.8027\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3307 - acc: 0.8849 - val_loss: 0.5627 - val_acc: 0.8050\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3298 - acc: 0.8855 - val_loss: 0.5620 - val_acc: 0.8047\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3281 - acc: 0.8857 - val_loss: 0.5629 - val_acc: 0.8050\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3273 - acc: 0.8852 - val_loss: 0.5648 - val_acc: 0.8027\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3255 - acc: 0.8857 - val_loss: 0.5655 - val_acc: 0.8043\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3241 - acc: 0.8864 - val_loss: 0.5688 - val_acc: 0.8000\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3235 - acc: 0.8878 - val_loss: 0.5679 - val_acc: 0.8023\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3217 - acc: 0.8878 - val_loss: 0.5675 - val_acc: 0.8063\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3204 - acc: 0.8885 - val_loss: 0.5685 - val_acc: 0.8073\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3193 - acc: 0.8899 - val_loss: 0.5714 - val_acc: 0.8030\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3181 - acc: 0.8891 - val_loss: 0.5703 - val_acc: 0.8050\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3169 - acc: 0.8891 - val_loss: 0.5727 - val_acc: 0.8037\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3152 - acc: 0.8909 - val_loss: 0.5719 - val_acc: 0.8020\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3144 - acc: 0.8909 - val_loss: 0.5724 - val_acc: 0.8027\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3131 - acc: 0.8907 - val_loss: 0.5738 - val_acc: 0.8030\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3118 - acc: 0.8916 - val_loss: 0.5753 - val_acc: 0.8000\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3107 - acc: 0.8922 - val_loss: 0.5772 - val_acc: 0.7990\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.3096 - acc: 0.8922 - val_loss: 0.5778 - val_acc: 0.8023\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.3084 - acc: 0.8935 - val_loss: 0.5824 - val_acc: 0.7990\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3070 - acc: 0.8942 - val_loss: 0.5815 - val_acc: 0.7987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3058 - acc: 0.8942 - val_loss: 0.5798 - val_acc: 0.8050\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3047 - acc: 0.8943 - val_loss: 0.5822 - val_acc: 0.8037\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3035 - acc: 0.8945 - val_loss: 0.5835 - val_acc: 0.8010\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 35us/step\n",
      "4000/4000 [==============================] - 0s 37us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.29896525318333594, 0.8974848484848484], [0.5769140058755875, 0.80875])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)\n",
    "results_train, results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, you were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). your test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, you not only built an initial deep-learning model, you then used a validation set to tune your model using various types of regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
